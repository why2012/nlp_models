{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Miniconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf \n",
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import threading\n",
    "from abc import ABCMeta, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(path=\"E:/paper/stackingmodel/imdb/imdb.npz\", num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWordsStatistics(sentences, sample = 1e-4):\n",
    "    from collections import namedtuple, defaultdict\n",
    "    WordStat = namedtuple(\"WordStat\", [\"count\", \"sample_int\"])\n",
    "    wordsStat = defaultdict(lambda: {\"count\": 0, \"sample_int\": sample})\n",
    "    retain_total = 0\n",
    "    for sent in sentences:\n",
    "        for word in sent:\n",
    "            wordsStat[word][\"count\"] += 1\n",
    "            retain_total += 1\n",
    "    threshold_count = sample * retain_total\n",
    "    for w in wordsStat:\n",
    "        v = wordsStat[w][\"count\"]\n",
    "        word_probability = (np.sqrt(v / threshold_count) + 1) * (threshold_count / v)\n",
    "        if word_probability > 1.0:\n",
    "            word_probability = 1.0\n",
    "        wordsStat[w][\"sample_int\"] = int(round(word_probability * 2**32))\n",
    "        wordsStat[w] = WordStat(count=wordsStat[w][\"count\"], sample_int=wordsStat[w][\"sample_int\"])\n",
    "    return wordsStat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordsStat = getWordsStatistics(np.hstack([X_train, X_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train_df = pd.DataFrame({\"description\": X_train, \"label\": y_train})\n",
    "# X_train_df.to_csv(\"E:/kaggle/avito/imdb_testset/train.data\", index=False)\n",
    "# X_test_df = pd.DataFrame({\"description\": X_test, \"label\": y_test})\n",
    "# X_test_df.to_csv(\"E:/kaggle/avito/imdb_testset/test.data\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SamplePool(metaclass=ABCMeta):\n",
    "    __metaclass__ = ABCMeta\n",
    "    \n",
    "    def __init__(self, chunk_size = 100):\n",
    "        self.__chunk_size = chunk_size\n",
    "        self.chunk_indices_list = np.zeros(chunk_size)\n",
    "    \n",
    "    @property\n",
    "    def chunk_size(self):\n",
    "        return self.__chunk_size\n",
    "    \n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "    def extend(self, samplepool_to_extend):\n",
    "        if not isinstance(samplepool_to_extend, SamplePool):\n",
    "            raise Exception(\"Illegal pool type\")\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __next__(self):\n",
    "        pass\n",
    "\n",
    "class InMemorySamplePool(SamplePool):\n",
    "    def __init__(self, samples, chunk_size = 1000):\n",
    "        super(InMemorySamplePool, self).__init__(chunk_size=chunk_size)\n",
    "        if not isinstance(samples, np.ndarray):\n",
    "            raise Exception(\"samples must be an ndarray\")\n",
    "        self.samples = samples\n",
    "        self.iter_index = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.iter_index = 0\n",
    "    \n",
    "    def extend(self, samplepool_to_extend):\n",
    "        super(InMemorySamplePool, self).extend(samplepool_to_extend)\n",
    "        self.samples = np.concatenate([self.samples, samplepool_to_extend.samples])\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.iter_index + self.chunk_size <= len(self.samples):\n",
    "            iter_samples = self.samples[self.iter_index: self.iter_index + self.chunk_size]\n",
    "            self.chunk_indices_list[:] = list(range(self.iter_index, self.iter_index + self.chunk_size))\n",
    "            self.iter_index = self.iter_index + self.chunk_size\n",
    "        else:\n",
    "            iter_samples_1 = self.samples[self.iter_index:]\n",
    "            chunk_indices_1 = list(range(self.iter_index, len(self.samples)))\n",
    "            self.iter_index = (self.iter_index + self.chunk_size) % len(self.samples)\n",
    "            iter_samples_2 = self.samples[:self.iter_index]\n",
    "            chunk_indices_2 = list(range(0, self.iter_index))\n",
    "            iter_samples = np.concatenate([iter_samples_1, iter_samples_2]) \n",
    "            chunk_indices = np.concatenate([chunk_indices_1, chunk_indices_2]) \n",
    "            self.chunk_indices_list[:] = chunk_indices\n",
    "        return iter_samples\n",
    "\n",
    "# file must be saved as csv with header \n",
    "class OutMemorySamplePool(SamplePool):\n",
    "    def __init__(self, sample_filepath_list, target_col_name, chunk_size = 1000):\n",
    "        super(OutMemorySamplePool, self).__init__(chunk_size=chunk_size)\n",
    "        if sample_filepath_list is None or not isinstance(sample_filepath_list, list) or len(sample_filepath_list) == 0:\n",
    "            raise Exception(\"filepath list is empty or not a list\")\n",
    "        self.sample_filepath_list = sample_filepath_list\n",
    "        self.target_col_name = target_col_name\n",
    "        self.dataframe_iter = None\n",
    "        self.dataframe_iter_index = 0\n",
    "        self.__current_chunk = None\n",
    "        self.chunk_cumulation = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.dataframe_iter = None\n",
    "        self.dataframe_iter_index = 0\n",
    "        self.__current_chunk = None\n",
    "    \n",
    "    def extend(self, samplepool_to_extend):\n",
    "        super(OutMemorySamplePool, self).extend(samplepool_to_extend)\n",
    "        self.sample_filepath_list.extend(samplepool_to_extend.sample_filepath_list)\n",
    "    \n",
    "    def __get_chunk__(self):\n",
    "        current_chunk_size = None\n",
    "        while(1):\n",
    "            if self.dataframe_iter is None:\n",
    "                self.dataframe_iter = pd.read_csv(self.sample_filepath_list[self.dataframe_iter_index], usecols=[self.target_col_name], iterator=True, chunksize=self.chunk_size, encoding=\"utf-8\")\n",
    "                self.dataframe_iter_index = (self.dataframe_iter_index + 1) % len(self.sample_filepath_list)\n",
    "            try:\n",
    "                if self.__current_chunk is not None:\n",
    "                    extra_chunk_size = self.chunk_size - len(self.__current_chunk)\n",
    "                    extra_chunk = self.dataframe_iter.get_chunk(extra_chunk_size).to_dict(\"list\")[self.target_col_name]\n",
    "                    current_chunk = np.concatenate([self.__current_chunk, extra_chunk])\n",
    "                    current_chunk_size = len(self.__current_chunk)\n",
    "                    self.__current_chunk = None\n",
    "                else:\n",
    "                    current_chunk = self.dataframe_iter.get_chunk().to_dict(\"list\")[self.target_col_name]\n",
    "                if len(current_chunk) < self.chunk_size:\n",
    "                    self.__current_chunk = current_chunk\n",
    "                    self.dataframe_iter = None\n",
    "                    continue\n",
    "                else:\n",
    "                    if self.dataframe_iter_index - 1 == 0 and current_chunk_size is not None:\n",
    "                        chunk_indices_1 = list(range(self.chunk_cumulation, self.chunk_cumulation + current_chunk_size))\n",
    "                        chunk_indices_2 = list(range(0, extra_chunk_size))\n",
    "                        self.chunk_indices_list[:] = np.concatenate([chunk_indices_1, chunk_indices_2])\n",
    "                        self.chunk_cumulation = extra_chunk_size\n",
    "                        current_chunk_size = None\n",
    "                    else:\n",
    "                        self.chunk_indices_list[:] = list(range(self.chunk_cumulation, self.chunk_cumulation + self.chunk_size))\n",
    "                        self.chunk_cumulation += self.chunk_size\n",
    "            except StopIteration:\n",
    "                self.dataframe_iter.close()\n",
    "                self.dataframe_iter = None\n",
    "                if self.dataframe_iter_index == 0:\n",
    "                    self.chunk_cumulation = 0\n",
    "                continue\n",
    "            return current_chunk\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.__get_chunk__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AbstractGenerator(metaclass=ABCMeta):\n",
    "    __metaclass__ = ABCMeta\n",
    "    \n",
    "    @abstractmethod\n",
    "    def batch_index(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __iter__(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __next__(self):\n",
    "        pass\n",
    "\n",
    "class ParagraphVectorGenerator(AbstractGenerator):\n",
    "    def __init__(self, sample_pool, window_size = 10, batch_size = 10, max_nbatch = 5, unkown_word_indicator = 0, \n",
    "                 shuffle = False, idtype = np.int32, wordsStat = None, seed = 0):\n",
    "        if not isinstance(sample_pool, SamplePool):\n",
    "            raise Exception(\"sample_pool must be an instance of SamplePool\")\n",
    "        if window_size < 2:\n",
    "            raise Exception(\"window_size must be greater than or equal to 2\")\n",
    "        self.sample_pool = sample_pool\n",
    "        self.window_size = window_size\n",
    "        self.batch_size = batch_size\n",
    "        self.max_nbatch = max_nbatch\n",
    "        self._batch_index = 0\n",
    "        self._chunks = None\n",
    "        self._paragraph_index = 0\n",
    "        self._word_index = 0\n",
    "        self.unkown_word_indicator = unkown_word_indicator\n",
    "        self.shuffle = shuffle\n",
    "        self.idtype = idtype\n",
    "        self.wordsStat = wordsStat\n",
    "        self.seed = seed\n",
    "        self.random =  np.random.RandomState(self.seed)\n",
    "        self.__last_paragraph_index = -1\n",
    "        self.__last_paragraph_words = None\n",
    "    \n",
    "    def extend(self, generator_to_extend):\n",
    "        if not isinstance(generator_to_extend, ParagraphVectorGenerator):\n",
    "            raise Exception(\"Illegal generator type\")\n",
    "        self.sample_pool.extend(generator_to_extend.sample_pool)\n",
    "        \n",
    "    @property\n",
    "    def batch_index(self):\n",
    "        return self._batch_index\n",
    "    \n",
    "    def reset(self):\n",
    "        self.sample_pool.reset()\n",
    "        self._batch_index = 0\n",
    "        self._chunks = None\n",
    "        self._paragraph_index = 0\n",
    "        self._word_index = 0\n",
    "        \n",
    "    def check_and_prepad(self):\n",
    "        if self.__last_paragraph_index == self._paragraph_index:\n",
    "            return self.__last_paragraph_words\n",
    "        \n",
    "        current_chunk_item = self._chunks[self._paragraph_index]\n",
    "        if isinstance(current_chunk_item, str):\n",
    "            if current_chunk_item.startswith(\"nan\"):\n",
    "                current_chunk_item = \"np.nan\"\n",
    "            current_chunk_item = eval(current_chunk_item)\n",
    "        if current_chunk_item is np.nan:\n",
    "            current_chunk_item = []\n",
    "        if self.wordsStat is not None:\n",
    "            current_chunk_item = [w for w in current_chunk_item if w in self.wordsStat and \n",
    "                                  self.wordsStat[w].sample_int > self.random.rand() * 2 ** 32]\n",
    "        if len(current_chunk_item) < self.window_size:\n",
    "            # padding with unkown word indicator\n",
    "            padding_size = self.window_size - len(current_chunk_item)\n",
    "            current_chunk_item = np.pad(current_chunk_item, [(0, padding_size)], \"constant\", constant_values=[self.unkown_word_indicator] * 2)\n",
    "        \n",
    "        self.__last_paragraph_index = self._paragraph_index\n",
    "        self.__last_paragraph_words = current_chunk_item\n",
    "        \n",
    "        return current_chunk_item\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.max_nbatch != -1:\n",
    "            self._batch_index += 1\n",
    "            if self._batch_index > self.max_nbatch:\n",
    "                raise StopIteration()\n",
    "        return self.generate_batch()\n",
    "    \n",
    "# distributed memory model generator\n",
    "class DMGenerator(ParagraphVectorGenerator):\n",
    "    def generate_batch(self):\n",
    "        current_batches = np.zeros((self.batch_size, self.window_size - 1), dtype=self.idtype)\n",
    "        current_labels = np.zeros((self.batch_size, 1), dtype=self.idtype)\n",
    "        current_paragraph_indices = np.zeros((self.batch_size, 1), dtype=self.idtype)\n",
    "        for loop_i in range(self.batch_size):\n",
    "            if self._paragraph_index == 0 and self._word_index == 0:\n",
    "                self._chunks = self.sample_pool.__next__()\n",
    "            current_paragraph = self.check_and_prepad()\n",
    "            current_batch = current_paragraph[self._word_index: self._word_index + self.window_size - 1].copy()\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(current_batch)\n",
    "            current_batches[loop_i, :] = current_batch\n",
    "            current_labels[loop_i, :] = current_paragraph[self._word_index + self.window_size - 1]\n",
    "            current_paragraph_indices[loop_i, :] = self.sample_pool.chunk_indices_list[self._paragraph_index]\n",
    "            self._word_index += 1\n",
    "            if self._word_index + self.window_size - 1 == len(current_paragraph):\n",
    "                self._paragraph_index = (self._paragraph_index + 1) % self.sample_pool.chunk_size\n",
    "                self._word_index = 0\n",
    "        if self.shuffle:\n",
    "            perm = np.random.permutation(current_batches.shape[0])\n",
    "            current_batches = current_batches[perm]\n",
    "            current_labels = current_labels[perm]\n",
    "            current_paragraph_indices = current_paragraph_indices[perm]\n",
    "        return current_batches, current_labels, current_paragraph_indices\n",
    "\n",
    "# bag of word model generator\n",
    "class BOWGenerator(ParagraphVectorGenerator):\n",
    "    def generate_batch(self):\n",
    "        current_batches = np.zeros((self.batch_size, self.window_size), dtype=self.idtype)\n",
    "        current_paragraph_indices = np.zeros((self.batch_size, 1), dtype=self.idtype)\n",
    "        for loop_i in range(self.batch_size):\n",
    "            if self._paragraph_index == 0 and self._word_index == 0:\n",
    "                self._chunks = self.sample_pool.__next__()\n",
    "            current_paragraph = self.check_and_prepad()\n",
    "            current_batch = current_paragraph[self._word_index: self._word_index + self.window_size].copy()\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(current_batch)\n",
    "            current_batches[loop_i, :] = current_batch\n",
    "            current_paragraph_indices[loop_i, :] = self.sample_pool.chunk_indices_list[self._paragraph_index]\n",
    "            self._word_index += 1\n",
    "            if self._word_index + self.window_size - 1 == len(current_paragraph):\n",
    "                self._paragraph_index = (self._paragraph_index + 1) % self.sample_pool.chunk_size\n",
    "                self._word_index = 0\n",
    "        if self.shuffle:\n",
    "            perm = np.random.permutation(current_batches.shape[0])\n",
    "            current_batches = current_batches[perm]\n",
    "            current_paragraph_indices = current_paragraph_indices[perm]\n",
    "        return current_batches, current_paragraph_indices\n",
    "    \n",
    "# continuous bag of word model generator\n",
    "class CBOWGenerator(ParagraphVectorGenerator):\n",
    "    def generate_batch(self):\n",
    "        current_batches = np.zeros((self.batch_size, self.window_size - 1), dtype=self.idtype)\n",
    "        current_labels = np.zeros((self.batch_size, 1), dtype=self.idtype)\n",
    "        current_paragraph_indices = np.zeros((self.batch_size, 1), dtype=self.idtype)\n",
    "        for loop_i in range(self.batch_size):\n",
    "            if self._paragraph_index == 0 and self._word_index == 0:\n",
    "                self._chunks = self.sample_pool.__next__()\n",
    "            current_paragraph = self.check_and_prepad()\n",
    "            current_batch = current_paragraph[self._word_index: self._word_index + self.window_size]\n",
    "            target_word = current_batch[int(self.window_size / 2)]\n",
    "            # copy a new one\n",
    "            current_batch = np.delete(current_batch, [int(self.window_size / 2)])\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(current_batch)\n",
    "            current_batches[loop_i, :] = current_batch\n",
    "            current_labels[loop_i, :] = target_word\n",
    "            current_paragraph_indices[loop_i, :] = self.sample_pool.chunk_indices_list[self._paragraph_index]\n",
    "            self._word_index += 1\n",
    "            if self._word_index + self.window_size - 1 == len(current_paragraph):\n",
    "                self._paragraph_index = (self._paragraph_index + 1) % self.sample_pool.chunk_size\n",
    "                self._word_index = 0\n",
    "        if self.shuffle:\n",
    "            perm = np.random.permutation(current_batches.shape[0])\n",
    "            current_batches = current_batches[perm]\n",
    "            current_labels = current_labels[perm]\n",
    "            current_paragraph_indices = current_paragraph_indices[perm]\n",
    "        return current_batches, current_labels, current_paragraph_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imPool = InMemorySamplePool(np.array([[5],[0,1,2,3,4],[1,2,3,4,5],[2,3,4,5,6],[3,4,5,6,7,8],[4,5,6],\n",
    "                                      np.nan, [5,6,7,8],[6,7,8,9,10,11],[7,8,9,10,11],[8,9,10],[9,10,11,12]]), chunk_size=5)\n",
    "omPoolNumber = OutMemorySamplePool([\"E:/kaggle/avito/preprocessing/test.data\", \"E:/kaggle/avito/preprocessing/test2.data\"], target_col_name=\"c\", chunk_size=5)\n",
    "omPool = OutMemorySamplePool([\"E:/kaggle/avito/preprocessing/train_descriptions.data\", \"E:/kaggle/avito/preprocessing/train_active_descriptions.data\"], target_col_name=\"description\", chunk_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dmgener = DMGenerator(omPoolNumber, window_size=3, batch_size=5, max_nbatch=-1)\n",
    "bowgener = BOWGenerator(omPoolNumber, window_size=3, batch_size=5, max_nbatch=-1)\n",
    "cbowgener = CBOWGenerator(omPoolNumber, window_size=3, batch_size=5, max_nbatch=-1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cbowgener.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdbImTrainingPool = InMemorySamplePool(X_train, chunk_size=1000)\n",
    "imdbOmTrainingPool = OutMemorySamplePool([\"E:/kaggle/avito/imdb_testset/train.data\"], target_col_name=\"description\", chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdbDmGener = DMGenerator(imdbImTrainingPool, window_size=8, batch_size=256, max_nbatch=-1)\n",
    "imdbBowGener = BOWGenerator(imdbImTrainingPool, window_size=8, batch_size=256, max_nbatch=-1)\n",
    "imdbCBowGener = CBOWGenerator(imdbImTrainingPool, window_size=11, batch_size=1, max_nbatch=-1, shuffle=False, wordsStat=wordsStat)\n",
    "\n",
    "imdbDmGener_Om = DMGenerator(imdbOmTrainingPool, window_size=8, batch_size=256, max_nbatch=-1)\n",
    "imdbBowGener_Om = BOWGenerator(imdbOmTrainingPool, window_size=8, batch_size=256, max_nbatch=-1)\n",
    "imdbCBowGener_Om = CBOWGenerator(imdbOmTrainingPool, window_size=8, batch_size=256, max_nbatch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 973, 1622, 1385,   65,  458,   66, 3941,  173,  256,  100]]),\n",
       " array([[4468]]),\n",
       " array([[0]]))"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdbCBowGener.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paragraph vector distributed memory\n",
    "class ParagraphVectorDmNet(object):\n",
    "    def __init__(self, word_embedding_size = 400, paragraph_embedding_size = 400, q_batch_size = 256, batch_size = 256,\n",
    "                 vocabulary_size = 10000, paragraph_size = 25000, window_size = 8, mode = \"concat\", wordsStat = None, worker = -1,\n",
    "                 contrast_num_samples = 64, optimizer_type = tf.train.AdagradOptimizer, optimizer_kwargs = {}, eval_epochs = 1000):\n",
    "        self.word_embedding_size = word_embedding_size\n",
    "        self.paragraph_embedding_size = paragraph_embedding_size\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.paragraph_size = paragraph_size\n",
    "        self.window_size = window_size\n",
    "        self.q_batch_size = q_batch_size\n",
    "        self.batch_size = batch_size\n",
    "        if worker == -1 and q_batch_size != batch_size:\n",
    "            raise Exception(\"q_batch_size must be equal to batch_size when worker = -1\")\n",
    "        self.mode = mode\n",
    "        self.wordsStat = wordsStat\n",
    "        if self.mode == \"total_average\" and self.word_embedding_size != self.paragraph_embedding_size:\n",
    "            raise Exception(\"word_embedding_size != paragraph_embedding_size in total_average mode\")\n",
    "        self.contrast_num_samples = contrast_num_samples\n",
    "        self.nce_weights = None\n",
    "        self.nce_biases = None\n",
    "        self.optimizer_type = optimizer_type\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self.worker = worker\n",
    "        self.eval_epochs = eval_epochs\n",
    "        self.force_build_net = True\n",
    "        self.tensor_map = None\n",
    "        \n",
    "    def build_net(self, learning_rate = 1.0, training = True, nce_weights = None, nce_biases = None, word_embeddings = None):\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            tf_queue = tf.FIFOQueue(self.q_batch_size * 2, \n",
    "                                    dtypes=[tf.int32, tf.int32, tf.int32], \n",
    "                                    shapes=[[self.window_size - 1], [1], [1]])\n",
    "            train_inputs, train_labels, paragraph_inputs = tf_queue.dequeue_many(self.batch_size)\n",
    "            \n",
    "            if training:\n",
    "                word_embeddings = tf.Variable(tf.random_uniform([self.vocabulary_size, self.word_embedding_size], -1.0, 1.0))\n",
    "            else:\n",
    "                if word_embeddings is None:\n",
    "                    raise Exception(\"word_embeddings cannot be None in testing stage\")\n",
    "                word_embeddings = tf.constant(word_embeddings)\n",
    "            paragraph_embeddings = tf.Variable(tf.random_uniform([self.paragraph_size, self.paragraph_embedding_size], -1.0, 1.0))\n",
    "            \n",
    "            # shape: (None, window_size - 1, w_embed_size)\n",
    "            word_embed = tf.nn.embedding_lookup(word_embeddings, train_inputs)\n",
    "            # shape: (None, 1, p_embed_size)\n",
    "            paragraph_embed = tf.nn.embedding_lookup(paragraph_embeddings, paragraph_inputs)\n",
    "            \n",
    "            if self.mode == \"concat\":\n",
    "                # shape: (None, (window_size - 1) * w_embed_size)\n",
    "                word_embed = tf.reshape(word_embed, shape=(-1, word_embed.shape[1] * word_embed.shape[2]))\n",
    "                # shape: (None, p_embed_size)\n",
    "                paragraph_embed = tf.squeeze(paragraph_embed, [1])\n",
    "                # shape: (None, (window_size - 1) * w_embed_size + p_embed_size)\n",
    "                train_embed = tf.concat([paragraph_embed, word_embed], axis=1)\n",
    "            elif self.mode == \"average\":\n",
    "                # shape: (None, w_embed_size)\n",
    "                word_embed = tf.reduce_mean(word_embed, axis=1)\n",
    "                # shape: (None, p_embed_size)\n",
    "                paragraph_embed = tf.squeeze(paragraph_embed, [1])\n",
    "                train_embed = tf.concat([paragraph_embed, word_embed], axis=1)\n",
    "            elif self.mode == \"total_average\":\n",
    "                train_embed = tf.concat([paragraph_embed, word_embed], axis=1)\n",
    "                train_embed = tf.reduce_mean(train_embed, axis=1)\n",
    "            elif self.mode == \"sum_concat\":\n",
    "                # shape: (None, w_embed_size)\n",
    "                word_embed = tf.reduce_sum(word_embed, axis=1)\n",
    "                # shape: (None, p_embed_size)\n",
    "                paragraph_embed = tf.squeeze(paragraph_embed, [1])\n",
    "                train_embed = tf.concat([paragraph_embed, word_embed], axis=1)\n",
    "            else:\n",
    "                raise NotImplementedError(\"unknown mode\")\n",
    "            \n",
    "            embedding_size = train_embed.shape[1].value\n",
    "            if training:\n",
    "                nce_weights = tf.Variable(tf.truncated_normal([self.vocabulary_size, embedding_size], stddev=1.0 / np.sqrt(embedding_size)))\n",
    "                nce_biases = tf.Variable(tf.zeros([self.vocabulary_size]))\n",
    "            else:\n",
    "                if nce_weights is None or nce_biases is None:\n",
    "                    raise Exception(\"nce_weights and nce_biases cannot be None in testing stage\")\n",
    "                nce_weights = tf.constant(nce_weights)\n",
    "                nce_biases = tf.constant(nce_biases)\n",
    "            losses = tf.nn.nce_loss(weights=nce_weights,\n",
    "                                                 biases=nce_biases,\n",
    "                                                 labels=train_labels,\n",
    "                                                 inputs=train_embed,\n",
    "                                                 num_sampled=self.contrast_num_samples,\n",
    "                                                 num_classes=self.vocabulary_size)\n",
    "            loss = tf.reduce_mean(losses)\n",
    "            optimizer = self.optimizer_type(learning_rate, **self.optimizer_kwargs)\n",
    "            trainable_vars = tf.trainable_variables()\n",
    "            gradients = []\n",
    "            for i in range(self.batch_size):\n",
    "                grads_and_vars = optimizer.compute_gradients(losses[i], trainable_vars)\n",
    "                gradients.extend(grads_and_vars)\n",
    "            train_op = optimizer.apply_gradients(gradients)\n",
    "#             train_op = optimizer.minimize(loss)\n",
    "            init = tf.global_variables_initializer()\n",
    "        return {\"train_inputs\": train_inputs, \"train_labels\": train_labels, \"paragraph_inputs\": paragraph_inputs,\n",
    "               \"word_embeddings\": word_embeddings, \"paragraph_embeddings\": paragraph_embeddings,\n",
    "               \"train_op\": train_op, \"init\": init, \"graph\": graph, \"nce_weights\": nce_weights, \"nce_biases\": nce_biases,\n",
    "               \"loss\": loss, \"tf_queue\": tf_queue}\n",
    "    \n",
    "    def _train_op(self, trainingGenerator, session, tensor_map, mode = \"single_thread\"):\n",
    "        if mode == \"single_thread\":\n",
    "            return self._train_op_singlethread(trainingGenerator, session, tensor_map)\n",
    "        else:\n",
    "            return self._train_op_multithread(trainingGenerator, session, tensor_map)\n",
    "    \n",
    "    def _train_op_singlethread(self, trainingGenerator, session, tensor_map):\n",
    "        average_loss = 0\n",
    "        for step, (current_batches, current_labels, current_paragraph_indices) in enumerate(trainingGenerator):\n",
    "            tensor_map[\"tf_queue\"].enqueue_many([current_batches, current_labels, current_paragraph_indices]).run()\n",
    "            for train_inner_step in range(int(np.ceil(self.q_batch_size / self.batch_size))):\n",
    "                _, loss_val = session.run([tensor_map[\"train_op\"], tensor_map[\"loss\"]])\n",
    "                average_loss += loss_val\n",
    "            if (step % self.eval_epochs == 0 and step > 0) :\n",
    "                average_loss /= self.eval_epochs\n",
    "                print(\"Average loss at step\" % trainingGenerator.max_nbatch, step, \"/%s: \", average_loss)\n",
    "                average_loss = 0\n",
    "    \n",
    "    def _train_op_multithread(self, trainingGenerator, session, tensor_map):\n",
    "        def _multithread_do_train(thread_id, coord, model, session, tensor_map):\n",
    "            step = 0\n",
    "            log_step = 0\n",
    "            average_loss = 0\n",
    "            while 1:\n",
    "                try:\n",
    "                    _, loss_val = session.run([tensor_map[\"train_op\"], tensor_map[\"loss\"]])\n",
    "                    average_loss += loss_val\n",
    "                    step += 1\n",
    "                    if step * model.batch_size >= model.q_batch_size * model.eval_epochs:\n",
    "                        average_loss /= step\n",
    "                        log_step += 1\n",
    "                        print(\"[Thread-%s] Average loss at step %s: %s\" % (thread_id, log_step, average_loss))\n",
    "                        average_loss = 0\n",
    "                        step = 0\n",
    "                except:\n",
    "                    # queue closed, and no more elements left\n",
    "                    break\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = [threading.Thread(target=_multithread_do_train, args=(i, coord, self, session, tensor_map)) for i in range(self.worker)]\n",
    "        for t in threads:\n",
    "            t.deamon = True\n",
    "            t.start()\n",
    "        for step, (current_batches, current_labels, current_paragraph_indices) in enumerate(trainingGenerator):\n",
    "            tensor_map[\"tf_queue\"].enqueue_many([current_batches, current_labels, current_paragraph_indices]).run()\n",
    "        coord.request_stop()\n",
    "        tensor_map[\"tf_queue\"].close().run()\n",
    "        coord.join(threads)\n",
    "    \n",
    "    def _fit(self, trainingGenerator, learning_rate = 1.0, epochs = None, return_embeddings = False, training = True):\n",
    "        trainingGenerator.reset()\n",
    "        if epochs is not None:\n",
    "            trainingGenerator.max_nbatch = epochs\n",
    "        trainingGenerator.batch_size = self.q_batch_size\n",
    "        trainingGenerator.wordsStat = self.wordsStat\n",
    "        trainingGenerator.window_size = self.window_size\n",
    "        if self.force_build_net or self.tensor_map is None:\n",
    "            if training:\n",
    "                self.tensor_map = self.build_net(learning_rate=learning_rate)\n",
    "            else:\n",
    "                self.tensor_map = self.build_net(learning_rate=learning_rate, training=False, \n",
    "                                        nce_weights=self.nce_weights, nce_biases=self.nce_biases,\n",
    "                                        word_embeddings=self.word_embeddings)\n",
    "        tensor_map = self.tensor_map\n",
    "        session = tf.Session(graph=tensor_map[\"graph\"])\n",
    "        self._session = session\n",
    "        with session:\n",
    "            tensor_map[\"init\"].run()\n",
    "            print(\"Initialized\")\n",
    "            if self.worker > 0:\n",
    "                self._train_op(trainingGenerator, session, tensor_map, mode=\"multi_thread\")\n",
    "            else:\n",
    "                self._train_op(trainingGenerator, session, tensor_map, mode=\"single_thread\")\n",
    "            if training:\n",
    "                self.nce_weights = tensor_map[\"nce_weights\"].eval()\n",
    "                self.nce_biases = tensor_map[\"nce_biases\"].eval()\n",
    "                self.word_embeddings = tensor_map[\"word_embeddings\"].eval()\n",
    "            if return_embeddings:\n",
    "                embeddings = {\"paragraph_embeddings\": tensor_map[\"paragraph_embeddings\"].eval(),\n",
    "                              \"word_embeddings\": tensor_map[\"word_embeddings\"].eval()}\n",
    "                return embeddings\n",
    "    \n",
    "    def fit(self, trainingGenerator, learning_rate = 1.0, epochs = None):\n",
    "        self._fit(trainingGenerator=trainingGenerator, learning_rate=learning_rate, epochs=epochs, return_embeddings=False, training=True)\n",
    "    \n",
    "    def transform(self, testingGenerator, learning_rate = 1.0, epochs = None):\n",
    "        return self._fit(trainingGenerator=testingGenerator, learning_rate=learning_rate, epochs=epochs, return_embeddings=True, training=False)\n",
    "    \n",
    "    def fit_transform(self, trainingGenerator, learning_rate = 1.0, epochs = None):\n",
    "        return self._fit(trainingGenerator=trainingGenerator, learning_rate=learning_rate, epochs=epochs, return_embeddings=True, training=True)\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        np.save(file=filepath, arr=np.concatenate([self.nce_weights, self.nce_biases.reshape((-1, 1)), self.word_embeddings], axis=1))\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        nce_weights_and_bias_and_wordembed = np.load(file=filepath)\n",
    "        if self.mode == \"concat\":\n",
    "            embedding_size = self.paragraph_embedding_size + (self.window_size - 1) * self.word_embedding_size\n",
    "        elif self.mode == \"average\" or self.mode == \"sum_concat\":\n",
    "            embedding_size = self.paragraph_embedding_size + self.word_embedding_size\n",
    "        elif self.mode == \"total_average\":\n",
    "            embedding_size = self.paragraph_embedding_size\n",
    "        else:\n",
    "            raise NotImplementedError(\"unknown mode\")\n",
    "        self.nce_weights = nce_weights_and_bias_and_wordembed[:, :embedding_size]\n",
    "        self.nce_biases = nce_weights_and_bias_and_wordembed[:, embedding_size]\n",
    "        self.word_embeddings = nce_weights_and_bias_and_wordembed[:, embedding_size + 1: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdbDmTrainingGener = CBOWGenerator(InMemorySamplePool(X_train, chunk_size=1000))\n",
    "imdbDmTestingGener = CBOWGenerator(InMemorySamplePool(X_test, chunk_size=1000))\n",
    "imdbDmTrainingGener.extend(imdbDmTestingGener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdbDmTrainingGener.sample_pool.samples.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449518.0\n"
     ]
    }
   ],
   "source": [
    "def total_epochs(sample_num, total_sample_len, window_size, batch_size, round_num):\n",
    "    return np.ceil((total_sample_len - window_size * sample_num + sample_num) / batch_size * round_num)\n",
    "\n",
    "print(total_epochs(\n",
    "    sample_num=imdbDmTrainingGener.sample_pool.samples.shape[0], \n",
    "    total_sample_len=np.sum(list(map(lambda x:len(x), imdbDmTrainingGener.sample_pool.samples))),\n",
    "    window_size=11,\n",
    "    batch_size=50,\n",
    "    round_num=2\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paragraph_size = imdbDmTrainingGener.sample_pool.samples.shape[0]\n",
    "dmNet = ParagraphVectorDmNet(vocabulary_size=10000, paragraph_size=paragraph_size, window_size=11, mode=\"total_average\", batch_size=50, \n",
    "                             word_embedding_size=100, paragraph_embedding_size=100, contrast_num_samples=5, wordsStat=wordsStat,\n",
    "                             optimizer_type=tf.train.AdamOptimizer, q_batch_size=50, eval_epochs=100, worker=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step/450000 100 :  25.38000499725342\n",
      "Average loss at step/450000 200 :  25.6086026096344\n",
      "Average loss at step/450000 300 :  28.968923416137695\n",
      "Average loss at step/450000 400 :  29.61231204986572\n",
      "Average loss at step/450000 500 :  33.719533653259276\n",
      "Average loss at step/450000 600 :  35.50120421409607\n",
      "Average loss at step/450000 700 :  38.29745830535889\n",
      "Average loss at step/450000 800 :  42.62316241264343\n",
      "Average loss at step/450000 900 :  45.284265270233156\n",
      "Average loss at step/450000 1000 :  47.68463788986206\n",
      "Average loss at step/450000 1100 :  49.852623777389525\n",
      "Average loss at step/450000 1200 :  50.44102087020874\n",
      "Average loss at step/450000 1300 :  54.059986419677735\n",
      "Average loss at step/450000 1400 :  53.488341655731205\n",
      "Average loss at step/450000 1500 :  60.843355655670166\n",
      "Average loss at step/450000 1600 :  60.42912744522095\n",
      "Average loss at step/450000 1700 :  58.352783641815186\n",
      "Average loss at step/450000 1800 :  64.35190250396728\n",
      "Average loss at step/450000 1900 :  67.02987632751464\n",
      "Average loss at step/450000 2000 :  63.3690514087677\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-4d6ef675d497>\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, trainingGenerator, learning_rate, epochs)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainingGenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingGenerator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainingGenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-4d6ef675d497>\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, trainingGenerator, learning_rate, epochs, return_embeddings, training)\u001b[0m\n\u001b[0;32m    174\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingGenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"multi_thread\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingGenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"single_thread\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnce_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"nce_weights\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-4d6ef675d497>\u001b[0m in \u001b[0;36m_train_op\u001b[1;34m(self, trainingGenerator, session, tensor_map, mode)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_train_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainingGenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"single_thread\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"single_thread\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_op_singlethread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingGenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_op_multithread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingGenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-4d6ef675d497>\u001b[0m in \u001b[0;36m_train_op_singlethread\u001b[1;34m(self, trainingGenerator, session, tensor_map)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0maverage_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcurrent_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_paragraph_indices\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingGenerator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[0mtensor_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tf_queue\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menqueue_many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_paragraph_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mtrain_inner_step\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_batch_size\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensor_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"train_op\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m   2040\u001b[0m         \u001b[0mnone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2041\u001b[0m     \"\"\"\n\u001b[1;32m-> 2042\u001b[1;33m     \u001b[0m_run_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2044\u001b[0m \u001b[0m_gradient_registry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRegistry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gradient\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[1;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   4488\u001b[0m                        \u001b[1;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4489\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 4490\u001b[1;33m   \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1291\u001b[0m                 run_metadata):\n\u001b[0;32m   1292\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1293\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1294\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1352\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1353\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[1;32m-> 1354\u001b[1;33m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[0;32m   1355\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "%time training_vectors = dmNet.fit_transform(imdbDmTrainingGener, epochs=450000, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %time testing_vectors = dmNet.transform(imdbDmTestingGener, epochs=90001, learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dmNet.save(\"E:/kaggle/avito/imdb_testset/dmnet_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_acc_on_model(classifier, X, y, split_ratio = 0.3):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=split_ratio, random_state=0, stratify=y)\n",
    "    classifier.fit(train_x, train_y)\n",
    "    proba_y = classifier.predict_proba(valid_x)\n",
    "    print(\"acc: \", sum(np.argmax(proba_y, axis=1) == valid_y) / proba_y.shape[0])\n",
    "\n",
    "def check_acc_on_rf(X, y, split_ratio = 0.3):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier(**{'n_jobs': -1,'n_estimators': 500,'max_depth': None,'max_features' : 'sqrt','random_state': 0})\n",
    "    check_acc_on_model(classifier, X, y, split_ratio)\n",
    "    \n",
    "def check_acc_on_logit(X, y, split_ratio = 0.3):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    classifier = LogisticRegression()\n",
    "    check_acc_on_model(classifier, X, y, split_ratio)\n",
    "    \n",
    "def check_acc_on_mlp(X, y, split_ratio = 0.3):\n",
    "    import sklearn.neural_network\n",
    "    classifier = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=[1024], learning_rate=\"adaptive\")\n",
    "    check_acc_on_model(classifier, X, y, split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_vectors[\"paragraph_embeddings\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_acc_on_logit(training_vectors[\"paragraph_embeddings\"][:25000], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check_acc_on_logit(testing_vectors[\"paragraph_embeddings\"][:25000], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"E:/kaggle/avito/imdb_testset/training_paragraph_embeddings\", training_vectors[\"paragraph_embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_vectors(vectors, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    from sklearn.manifold import TSNE\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init=\"pca\", n_iter=5000)\n",
    "    low_dim_embs = tsne.fit_transform(vectors)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(low_dim_embs[:, 0], low_dim_embs[:, 1], c=np.array([\"b\", \"g\"])[labels])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf_queue = tf.FIFOQueue(10, dtypes=[tf.float32, tf.float32], shapes=[[1], [2]])\n",
    "tf_a, tf_b = tf_queue.dequeue_many(1)\n",
    "tf_ab = tf.concat([tf_a, tf_b], axis=1)\n",
    "tf_sum = tf.reduce_sum(tf_ab, axis=1)\n",
    "tf_mean = tf.div(tf_sum, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf_queue.enqueue_many(([[1], [2], [3]], [[2,3],[3,4],[4,5]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf_queue.close())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf_queue.dequeue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
