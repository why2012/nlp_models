{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf \n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization, Add\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Concatenate, AveragePooling1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'preprocessing' from 'D:\\\\jupyter-workdir\\\\nlp\\\\bg_fusion\\\\preprocessing.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import pickle\n",
    "wordCounter = preprocessing.WordCounter()\n",
    "if not osp.isfile(\"E:/kaggle/avito/imdb_testset/aclImdb_v1/words_counter_list\"):\n",
    "    wordCounter.fit([\"E:/kaggle/avito/imdb_testset/aclImdb_v1/train_test_neg.txt\", \"E:/kaggle/avito/imdb_testset/aclImdb_v1/train_test_pos.txt\"])\n",
    "    pickle.dump(wordCounter.words_list, open(\"E:/kaggle/avito/imdb_testset/aclImdb_v1/words_counter_list\", \"wb\"))\n",
    "else:\n",
    "    wordCounter.words_list = pickle.load(open(\"E:/kaggle/avito/imdb_testset/aclImdb_v1/words_counter_list\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    import gensim\n",
    "    if not osp.isfile(\"E:/kaggle/avito/imdb_testset/gensim_models/imdb_word2vec\"):\n",
    "        sentences = gensim.models.word2vec.PathLineSentences(\"E:/kaggle/avito/imdb_testset/aclImdb_v1/train_test/\")\n",
    "        model = gensim.models.Word2Vec(sentences, size=200, window=5, min_count=1, workers=8)\n",
    "        model.save(\"E:/kaggle/avito/imdb_testset/gensim_models/imdb_word2vec\")\n",
    "    else:\n",
    "        model = gensim.models.Word2Vec.load(\"E:/kaggle/avito/imdb_testset/gensim_models/imdb_word2vec\")\n",
    "    emdedings = wordCounter.get_pretrain_embedding(model, num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words count 181924\n"
     ]
    }
   ],
   "source": [
    "print(\"words count\", len(wordCounter.words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getdata(num_words=None):\n",
    "    state = np.random.RandomState(0)\n",
    "    X_train_pos = np.array(wordCounter.transform([\"E:/kaggle/avito/imdb_testset/aclImdb_v1/train_pos.txt\"], max_words=num_words))\n",
    "    X_train_neg = np.array(wordCounter.transform([\"E:/kaggle/avito/imdb_testset/aclImdb_v1/train_neg.txt\"], max_words=num_words))\n",
    "    y_train_pos = np.ones(X_train_pos.shape[0])\n",
    "    y_train_neg = np.zeros(X_train_neg.shape[0])\n",
    "    X_test_pos = np.array(wordCounter.transform([\"E:/kaggle/avito/imdb_testset/aclImdb_v1/test_pos.txt\"], max_words=num_words))\n",
    "    X_test_neg = np.array(wordCounter.transform([\"E:/kaggle/avito/imdb_testset/aclImdb_v1/test_neg.txt\"], max_words=num_words))\n",
    "    y_test_pos = np.ones(X_test_pos.shape[0])\n",
    "    y_test_neg = np.zeros(X_test_neg.shape[0])\n",
    "    X_train, y_train = np.concatenate([X_train_pos, X_train_neg]), np.concatenate([y_train_pos, y_train_neg])\n",
    "    X_test, y_test = np.concatenate([X_test_pos, X_test_neg]), np.concatenate([y_test_pos, y_test_neg])\n",
    "    train_permut = state.permutation(X_train.shape[0])\n",
    "    test_permut = state.permutation(X_test.shape[0])\n",
    "    return (X_train[train_permut], y_train[train_permut]), (X_test[test_permut], y_test[test_permut])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (X_train, y_train), (X_test, y_test) = imdb.load_data(path=\"E:/paper/stackingmodel/imdb/imdb.npz\", num_words=10000)\n",
    "(X_train, y_train), (X_test, y_test) = getdata(num_words=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min([len(x) for X in (X_train, X_test) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SpatialPyramidPooling1D(Layer):\n",
    "    def __init__(self, pool_list, mode = \"max\", **kwargs):\n",
    "        self.pool_list = np.array(pool_list)\n",
    "        self.mode = mode\n",
    "        assert self.pool_list.ndim == 1, \"pool_list ndim must be 1\"\n",
    "        assert self.mode in [\"max\", \"avg\"], \"mode must be either max or avg\"\n",
    "        self.num_outputs = sum(pool_list)\n",
    "        super(SpatialPyramidPooling1D, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.gram_size = input_shape[2]\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.num_outputs * self.gram_size)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'pool_list': self.pool_list, \"mode\": self.mode}\n",
    "        base_config = super(SpatialPyramidPooling1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    def call(self, x):\n",
    "        input_shape = K.shape(x)\n",
    "        gram_length = [K.cast(input_shape[1], 'float32') / i for i in self.pool_list]\n",
    "        outputs = []\n",
    "        for pool_index, num_pool_regions in enumerate(self.pool_list):\n",
    "            for ix in range(num_pool_regions):\n",
    "                x1 = ix * gram_length[pool_index]\n",
    "                x2 = ix * gram_length[pool_index] + gram_length[pool_index]\n",
    "                x1 = K.cast(K.round(x1), 'int32')\n",
    "                x2 = K.cast(K.round(x2), 'int32')\n",
    "                # new_shape = [input_shape[0], x2 - x1, input_shape[2]]\n",
    "                x_crop = x[:, x1:x2, :]\n",
    "                # x_crop = K.reshape(x_crop, new_shape)\n",
    "                if self.mode == \"max\":\n",
    "                    pooled_val = K.max(x_crop, axis=1)\n",
    "                elif self.mode == \"avg\":\n",
    "                    pooled_val = K.mean(x_crop, axis=1)\n",
    "                outputs.append(pooled_val)\n",
    "        outputs = K.concatenate(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unsupport dynamic input size\n",
    "class KMaxPooling1D(Layer):\n",
    "    def __init__(self, pool_list, mode = \"max\", **kwargs):\n",
    "        self.pool_list = np.array(pool_list)\n",
    "        self.mode = mode\n",
    "        assert self.pool_list.ndim == 1, \"pool_list ndim must be 1\"\n",
    "        assert self.mode in [\"max\", \"avg\"], \"mode must be either max or avg\"\n",
    "        self.num_outputs = sum(pool_list)\n",
    "        super(KMaxPooling1D, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.gram_size = input_shape[2]\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.num_outputs * self.gram_size)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'pool_list': self.pool_list, \"mode\": self.mode}\n",
    "        base_config = super(KMaxPooling1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    def call(self, x):\n",
    "        input_shape = x.get_shape()\n",
    "        input_shape_list = input_shape.as_list()\n",
    "        x = tf.reshape(x, [-1, input_shape[1], input_shape[2], tf.constant(1)])\n",
    "        outputs = []\n",
    "        gram_length = [input_shape_list[1] / i for i in self.pool_list]\n",
    "        embedding_size = input_shape_list[2]\n",
    "        for pool_index, num_pool_regions in enumerate(self.pool_list):\n",
    "            ph = np.round(gram_length[pool_index]).astype(np.int32)\n",
    "            sh = ph\n",
    "            if self.mode == \"max\":\n",
    "                pool_result = tf.nn.max_pool(x,\n",
    "                                             ksize=[1, ph, embedding_size, 1], \n",
    "                                             strides=[1, sh, 1, 1],\n",
    "                                             padding='SAME')\n",
    "            elif self.mode == \"avg\":\n",
    "                pool_result = tf.nn.avg_pool(x,\n",
    "                                             ksize=[1, ph, embedding_size, 1], \n",
    "                                             strides=[1, sh, 1, 1],\n",
    "                                             padding='SAME')\n",
    "            outputs.append(tf.reshape(pool_result, [-1, tf.constant(self.pool_list[pool_index]) * input_shape[2]]))\n",
    "        outputs = K.concatenate(outputs, axis=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGenerator(object):\n",
    "    def __init__(self, X, y, batch_size = 125, bins_count=100, mode=\"train\", onehot = False):\n",
    "        self.batch_size = batch_size\n",
    "        padding_mode = \"random\" if mode == \"train\" else \"specific\"\n",
    "        self.pool = preprocessing.AutoPaddingInMemorySamplePool(X, chunk_size=batch_size, bins_count=bins_count, mode=padding_mode)\n",
    "        self.y = y[self.pool.sorted_indices]\n",
    "        self.mode = mode\n",
    "        self.y_indices_record = []\n",
    "        self.onehot = onehot\n",
    "        if self.onehot:\n",
    "            onehot_encoder = OneHotEncoder()\n",
    "            self.y = self.y.reshape((-1, 1))\n",
    "            self.y = onehot_encoder.fit_transform(self.y).toarray()\n",
    "    \n",
    "    def iter(self):\n",
    "        return self\n",
    "    \n",
    "    def reset(self):\n",
    "        self.pool.reset()\n",
    "        self.y_indices_record = []\n",
    "    \n",
    "    def __next__(self):\n",
    "        batch_samples = self.pool.__next__()\n",
    "        if self.mode == \"test\":\n",
    "            self.y_indices_record.extend(self.pool.chunk_indices_list)\n",
    "        return batch_samples, self.y[self.pool.chunk_indices_list]\n",
    "    \n",
    "    def get_test_y(self, steps):\n",
    "        return self.y[self.y_indices_record[: steps * self.batch_size]]\n",
    "onehot = True\n",
    "SentGener_train_all = SentenceGenerator(batch_size=64, X=X_train, y=y_train, mode=\"train\", onehot=onehot)\n",
    "SentGener_train = SentenceGenerator(batch_size=64, X=X_train[:22000], y=y_train[:22000], mode=\"train\", onehot=onehot)\n",
    "SentGener_val = SentenceGenerator(batch_size=64, X=X_train[22000:], y=y_train[22000:], mode=\"test\", bins_count=10, onehot=onehot)\n",
    "SentGener_test = SentenceGenerator(batch_size=64, X=X_test, y=y_test, mode=\"test\", onehot=onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 250 400 400 400 46\n"
     ]
    }
   ],
   "source": [
    "print(SentGener_train.pool.min_gap, SentGener_test.pool.min_gap, SentGener_train_all.pool.steps, SentGener_train.pool.steps, SentGener_test.pool.steps, min(SentGener_train.pool.bins_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping('val_loss', patience=5, mode=\"min\"), ModelCheckpoint(\"E:/kaggle/avito/imdb_testset/tf_model/spp_net_imdb.hdf5\", save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spp_embeding_model(pool_list, num_words, mode = \"avg\", word_size = 100, embedings = None):\n",
    "    word_indices = Input(shape=[None], name=\"word_indices\")\n",
    "    if emdedings is None:\n",
    "        word_embedding = Embedding(num_words, word_size)(word_indices)\n",
    "    else:\n",
    "        word_embedding = Embedding(num_words, word_size, weights=[embedings], trainable=False)(word_indices)\n",
    "    x_flow = word_embedding\n",
    "    x_flow = Dropout(0.1)(word_embedding)\n",
    "    x_flow = Conv1D(512, 3, padding='same', activation='relu', strides=1)(x_flow)\n",
    "    x_flow = MaxPooling1D(3)(x_flow)\n",
    "    x_flow = Conv1D(256, 3, padding='same', activation='relu', strides=1)(x_flow)\n",
    "    x_flow = Conv1D(128, 3, padding='same', activation='relu', strides=1)(x_flow)\n",
    "    x_flow = SpatialPyramidPooling1D(pool_list=pool_list, mode=mode)(x_flow)\n",
    "    x_flow = Dropout(0.1)(x_flow)\n",
    "    x_flow = Dense(512, activation='relu')(x_flow)\n",
    "    x_flow = Dropout(0.1)(x_flow)\n",
    "    x_flow = Dense(256, activation='relu')(x_flow)\n",
    "    y_output = Dense(1, activation='sigmoid')(x_flow)\n",
    "    sgd = Adam(lr=1e-3)\n",
    "    model = Model(inputs=[word_indices], outputs=y_output)\n",
    "    model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def get_spp_embeding_model_shallow(pool_list, num_words, mode = \"avg\", word_size = 100, embedings = None):\n",
    "    word_indices = Input(shape=[None], name=\"word_indices\")\n",
    "    if embedings is None:\n",
    "        word_embedding = Embedding(num_words, word_size)(word_indices)\n",
    "    else:\n",
    "        word_embedding = Embedding(num_words, word_size, weights=[embedings], trainable=False)(word_indices)\n",
    "    x_flow = word_embedding\n",
    "    # ==============================================================================\n",
    "    x_flow_a = Conv1D(128, 2, padding='same', activation='relu')(x_flow)\n",
    "    x_flow_b = Conv1D(128, 3, padding='same', activation='relu')(x_flow)\n",
    "    x_flow_c = Conv1D(128, 4, padding='same', activation='relu')(x_flow)\n",
    "    x_flow_d = Conv1D(128, 5, padding='same', activation='relu')(x_flow)\n",
    "    # ==============================================================================\n",
    "#     x_flow_a_a = Conv1D(128, 2, padding='same', activation='relu')(x_flow_a)\n",
    "#     x_flow_a_b = Conv1D(128, 3, padding='same', activation='relu')(x_flow_a)\n",
    "#     x_flow_a = Add()([x_flow_a_a, x_flow_a_b])\n",
    "    \n",
    "#     x_flow_b_a = Conv1D(128, 2, padding='same', activation='relu')(x_flow_b)\n",
    "#     x_flow_b_b = Conv1D(128, 3, padding='same', activation='relu')(x_flow_b)\n",
    "#     x_flow_b = Add()([x_flow_b_a, x_flow_b_b])\n",
    "    \n",
    "#     x_flow_c_a = Conv1D(128, 2, padding='same', activation='relu')(x_flow_c)\n",
    "#     x_flow_c_b = Conv1D(128, 3, padding='same', activation='relu')(x_flow_c)\n",
    "#     x_flow_c = Add()([x_flow_c_a, x_flow_c_b])\n",
    "    \n",
    "#     x_flow_d_a = Conv1D(128, 2, padding='same', activation='relu')(x_flow_d)\n",
    "#     x_flow_d_b = Conv1D(128, 3, padding='same', activation='relu')(x_flow_d)\n",
    "#     x_flow_d = Add()([x_flow_d_a, x_flow_d_b])\n",
    "    # ==============================================================================\n",
    "    x_flow = Concatenate(axis=1)([x_flow_a, x_flow_b, x_flow_c, x_flow_d])\n",
    "    x_flow = SpatialPyramidPooling1D(pool_list=pool_list, mode=mode)(x_flow)\n",
    "    x_flow = Dense(128, activation='relu')(x_flow)\n",
    "    y_output = Dense(2, activation='softmax')(x_flow)\n",
    "    # ==============================================================================\n",
    "    sgd = Adam(lr=1e-3)\n",
    "    model = Model(inputs=[word_indices], outputs=y_output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed = spp_model.layers[1]\n",
    "# K.get_session().run(embed.embeddings)\n",
    "# embedings = K.get_session().run(spp_model.layers[1].embeddings\n",
    "# np.save(\"E:/kaggle/avito/imdb_testset/imdb_embedding_size_300_words_30000\", embedings)\n",
    "# embedings = np.load(\"E:/kaggle/avito/imdb_testset/imdb_embedding_size_300_words_30000.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word_indices (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, None, 300)    9000000     word_indices[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, None, 128)    76928       embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, None, 128)    115328      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, None, 128)    153728      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, None, 128)    192128      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, None, 128)    0           conv1d_61[0][0]                  \n",
      "                                                                 conv1d_62[0][0]                  \n",
      "                                                                 conv1d_63[0][0]                  \n",
      "                                                                 conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_pyramid_pooling1d_7 (Sp (None, 896)          0           concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 128)          114816      spatial_pyramid_pooling1d_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 2)            258         dense_13[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 9,653,186\n",
      "Trainable params: 9,653,186\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "spp_model = get_spp_embeding_model_shallow(pool_list=[1, 2, 4], num_words=30000, word_size=300, mode=\"max\", embedings=None)\n",
    "spp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========round-0\n",
      "Epoch 1/5\n",
      "400/400 [==============================] - 22s 54ms/step - loss: 0.3449 - acc: 0.8335\n",
      "Epoch 2/5\n",
      "400/400 [==============================] - 20s 51ms/step - loss: 0.1070 - acc: 0.9623\n",
      "Epoch 3/5\n",
      "400/400 [==============================] - 22s 54ms/step - loss: 0.0291 - acc: 0.9910\n",
      "Epoch 4/5\n",
      "400/400 [==============================] - 21s 53ms/step - loss: 0.0038 - acc: 0.9991\n",
      "Epoch 5/5\n",
      "400/400 [==============================] - 20s 50ms/step - loss: 9.7635e-04 - acc: 1.0000\n",
      "400/400 [==============================] - 5s 14ms/step\n",
      "==========round-0, accuracy: 0.90004\n",
      "==========round-1\n",
      "Epoch 1/5\n",
      "400/400 [==============================] - 21s 52ms/step - loss: 1.5219e-04 - acc: 1.0000\n",
      "Epoch 2/5\n",
      "400/400 [==============================] - 21s 52ms/step - loss: 7.9442e-05 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "400/400 [==============================] - 21s 52ms/step - loss: 5.2793e-05 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "400/400 [==============================] - 21s 53ms/step - loss: 3.7307e-05 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "400/400 [==============================] - 21s 53ms/step - loss: 2.7365e-05 - acc: 1.0000\n",
      "400/400 [==============================] - 5s 13ms/step\n",
      "==========round-1, accuracy: 0.90128\n",
      "==========round-2\n",
      "Epoch 1/5\n",
      "400/400 [==============================] - 20s 49ms/step - loss: 2.1186e-05 - acc: 1.0000 0s - loss: 2.1190e-05 - acc: 1.0\n",
      "Epoch 2/5\n",
      "400/400 [==============================] - 21s 53ms/step - loss: 1.5723e-05 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "400/400 [==============================] - 21s 53ms/step - loss: 1.2515e-05 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "400/400 [==============================] - 19s 49ms/step - loss: 9.8141e-06 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 7.6899e-06 - acc: 1.0000\n",
      "400/400 [==============================] - 4s 11ms/step\n",
      "==========round-2, accuracy: 0.90256\n",
      "==========round-3\n",
      "Epoch 1/5\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 6.0772e-06 - acc: 1.0000\n",
      "Epoch 2/5\n",
      "400/400 [==============================] - 20s 49ms/step - loss: 4.8688e-06 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "400/400 [==============================] - 22s 54ms/step - loss: 3.8554e-06 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "400/400 [==============================] - 22s 54ms/step - loss: 3.0427e-06 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "400/400 [==============================] - 21s 52ms/step - loss: 2.4918e-06 - acc: 1.0000 2s\n",
      "400/400 [==============================] - 5s 12ms/step\n",
      "==========round-3, accuracy: 0.90264\n",
      "==========round-4\n",
      "Epoch 1/5\n",
      "400/400 [==============================] - 21s 52ms/step - loss: 1.9817e-06 - acc: 1.0000\n",
      "Epoch 2/5\n",
      "400/400 [==============================] - 21s 54ms/step - loss: 1.5510e-06 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 1.2661e-06 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "400/400 [==============================] - 20s 51ms/step - loss: 1.0575e-06 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "400/400 [==============================] - 22s 55ms/step - loss: 8.4967e-07 - acc: 1.0000\n",
      "400/400 [==============================] - 5s 14ms/step \n",
      "==========round-4, accuracy: 0.90276\n",
      "==========round-5\n",
      "Epoch 1/5\n",
      "400/400 [==============================] - 22s 54ms/step - loss: 6.9250e-07 - acc: 1.0000\n",
      "Epoch 2/5\n",
      "400/400 [==============================] - 21s 53ms/step - loss: 5.6276e-07 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "400/400 [==============================] - 21s 53ms/step - loss: 4.8559e-07 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "400/400 [==============================] - 21s 52ms/step - loss: 4.0299e-07 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "400/400 [==============================] - 22s 56ms/step - loss: 3.4146e-07 - acc: 1.0000\n",
      "400/400 [==============================] - 6s 14ms/step\n",
      "==========round-5, accuracy: 0.90284\n",
      "==========round-6\n",
      "Epoch 1/5\n",
      "400/400 [==============================] - 21s 53ms/step - loss: 2.9162e-07 - acc: 1.0000 1s - loss: 2.9274\n",
      "Epoch 2/5\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 2.5177e-07 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "400/400 [==============================] - 19s 49ms/step - loss: 2.1963e-07 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 1.9804e-07 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "400/400 [==============================] - 21s 53ms/step - loss: 1.7750e-07 - acc: 1.0000\n",
      "400/400 [==============================] - 5s 13ms/step\n",
      "==========round-6, accuracy: 0.90304\n",
      "==========round-7\n",
      "Epoch 1/5\n",
      "400/400 [==============================] - 20s 50ms/step - loss: 1.6483e-07 - acc: 1.0000\n",
      "Epoch 2/5\n",
      "400/400 [==============================] - 21s 52ms/step - loss: 1.5076e-07 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "400/400 [==============================] - 22s 55ms/step - loss: 1.4257e-07 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "400/400 [==============================] - 22s 56ms/step - loss: 1.3477e-07 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "400/400 [==============================] - 20s 50ms/step - loss: 1.2993e-07 - acc: 1.0000\n",
      "400/400 [==============================] - 5s 12ms/step\n",
      "==========round-7, accuracy: 0.90296\n",
      "==========round-8\n",
      "Epoch 1/5\n",
      "400/400 [==============================] - 20s 51ms/step - loss: 1.2625e-07 - acc: 1.0000\n",
      "Epoch 2/5\n",
      "400/400 [==============================] - 20s 49ms/step - loss: 1.2354e-07 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "400/400 [==============================] - 20s 49ms/step - loss: 1.2183e-07 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "400/400 [==============================] - 19s 47ms/step - loss: 1.2088e-07 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 1.2015e-07 - acc: 1.0000\n",
      "400/400 [==============================] - 4s 11ms/step\n",
      "==========round-8, accuracy: 0.9032\n",
      "==========round-9\n",
      "Epoch 1/5\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 1.1958e-07 - acc: 1.0000\n",
      "Epoch 2/5\n",
      "400/400 [==============================] - 19s 48ms/step - loss: 1.1961e-07 - acc: 1.0000\n",
      "Epoch 3/5\n",
      "400/400 [==============================] - 20s 50ms/step - loss: 1.1940e-07 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "400/400 [==============================] - 20s 51ms/step - loss: 1.1928e-07 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "400/400 [==============================] - 23s 57ms/step - loss: 1.1928e-07 - acc: 1.0000\n",
      "400/400 [==============================] - 6s 14ms/step\n",
      "==========round-9, accuracy: 0.90308\n"
     ]
    }
   ],
   "source": [
    "callbacks[0].best = np.inf\n",
    "callbacks[1].best = np.inf\n",
    "for i in range(10):\n",
    "    print(\"==========round-%s\" % i)\n",
    "    spp_model.fit_generator(SentGener_train_all, steps_per_epoch=SentGener_train_all.pool.steps, epochs=5, shuffle=True, verbose=1)\n",
    "    SentGener_test.reset()\n",
    "    test_y_hat = spp_model.predict_generator(SentGener_test, steps=SentGener_test.pool.steps, verbose=1)\n",
    "    acc = accuracy_score(SentGener_test.y, np.round(test_y_hat + 1e-5))\n",
    "    print(\"==========round-%s, accuracy: %s\" % (i, acc))\n",
    "# spp_model.load_weights(\"E:/kaggle/avito/imdb_testset/tf_model/spp_net_imdb.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 6s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "SentGener_test.reset()\n",
    "test_y_hat = spp_model.predict_generator(SentGener_test, steps=SentGener_test.pool.steps, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88996"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(SentGener_test.y, np.round(test_y_hat + 1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
