{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Miniconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf \n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'preprocessing' from 'D:\\\\jupyter-workdir\\\\nlp\\\\bg_fusion\\\\preprocessing.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import pickle\n",
    "wordCounter = preprocessing.WordCounter()\n",
    "if not osp.isfile(\"E:/kaggle/avito/imdb_testset/aclImdb_v1/words_counter_list\"):\n",
    "    wordCounter.fit([\"E:/kaggle/avito/imdb_testset/aclImdb_v1/train_test_neg.txt\", \"E:/kaggle/avito/imdb_testset/aclImdb_v1/train_test_pos.txt\"])\n",
    "    pickle.dump(wordCounter.words_list, open(\"E:/kaggle/avito/imdb_testset/aclImdb_v1/words_counter_list\", \"wb\"))\n",
    "else:\n",
    "    wordCounter.words_list = pickle.load(open(\"E:/kaggle/avito/imdb_testset/aclImdb_v1/words_counter_list\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    import gensim\n",
    "    if not osp.isfile(\"E:/kaggle/avito/imdb_testset/gensim_models/imdb_word2vec\"):\n",
    "        sentences = gensim.models.word2vec.PathLineSentences(\"E:/kaggle/avito/imdb_testset/aclImdb_v1/train_test/\")\n",
    "        model = gensim.models.Word2Vec(sentences, size=200, window=5, min_count=1, workers=8)\n",
    "        model.save(\"E:/kaggle/avito/imdb_testset/gensim_models/imdb_word2vec\")\n",
    "    else:\n",
    "        model = gensim.models.Word2Vec.load(\"E:/kaggle/avito/imdb_testset/gensim_models/imdb_word2vec\")\n",
    "    emdedings = wordCounter.get_pretrain_embedding(model, num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words count 181924\n"
     ]
    }
   ],
   "source": [
    "print(\"words count\", len(wordCounter.words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getdata(num_words=None):\n",
    "    state = np.random.RandomState(0)\n",
    "    X_train_pos = np.array(wordCounter.transform([\"E:/kaggle/avito/imdb_testset/aclImdb_v1/train_pos.txt\"], max_words=num_words))\n",
    "    X_train_neg = np.array(wordCounter.transform([\"E:/kaggle/avito/imdb_testset/aclImdb_v1/train_neg.txt\"], max_words=num_words))\n",
    "    y_train_pos = np.ones(X_train_pos.shape[0])\n",
    "    y_train_neg = np.zeros(X_train_neg.shape[0])\n",
    "    X_test_pos = np.array(wordCounter.transform([\"E:/kaggle/avito/imdb_testset/aclImdb_v1/test_pos.txt\"], max_words=num_words))\n",
    "    X_test_neg = np.array(wordCounter.transform([\"E:/kaggle/avito/imdb_testset/aclImdb_v1/test_neg.txt\"], max_words=num_words))\n",
    "    y_test_pos = np.ones(X_test_pos.shape[0])\n",
    "    y_test_neg = np.zeros(X_test_neg.shape[0])\n",
    "    X_train, y_train = np.concatenate([X_train_pos, X_train_neg]), np.concatenate([y_train_pos, y_train_neg])\n",
    "    X_test, y_test = np.concatenate([X_test_pos, X_test_neg]), np.concatenate([y_test_pos, y_test_neg])\n",
    "    train_permut = state.permutation(X_train.shape[0])\n",
    "    test_permut = state.permutation(X_test.shape[0])\n",
    "    return (X_train[train_permut], y_train[train_permut]), (X_test[test_permut], y_test[test_permut])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (X_train, y_train), (X_test, y_test) = imdb.load_data(path=\"E:/paper/stackingmodel/imdb/imdb.npz\", num_words=10000)\n",
    "(X_train, y_train), (X_test, y_test) = getdata(num_words=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min([len(x) for X in (X_train, X_test) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SpatialPyramidPooling1D(Layer):\n",
    "    def __init__(self, pool_list, mode = \"max\", **kwargs):\n",
    "        self.pool_list = np.array(pool_list)\n",
    "        self.mode = mode\n",
    "        assert self.pool_list.ndim == 1, \"pool_list ndim must be 1\"\n",
    "        assert self.mode in [\"max\", \"avg\"], \"mode must be either max or avg\"\n",
    "        self.num_outputs = sum(pool_list)\n",
    "        super(SpatialPyramidPooling1D, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.gram_size = input_shape[2]\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.num_outputs * self.gram_size)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'pool_list': self.pool_list, \"mode\": self.mode}\n",
    "        base_config = super(SpatialPyramidPooling1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    def call(self, x):\n",
    "        input_shape = K.shape(x)\n",
    "        gram_length = [K.cast(input_shape[1], 'float32') / i for i in self.pool_list]\n",
    "        outputs = []\n",
    "        for pool_index, num_pool_regions in enumerate(self.pool_list):\n",
    "            for ix in range(num_pool_regions):\n",
    "                x1 = ix * gram_length[pool_index]\n",
    "                x2 = ix * gram_length[pool_index] + gram_length[pool_index]\n",
    "                x1 = K.cast(K.round(x1), 'int32')\n",
    "                x2 = K.cast(K.round(x2), 'int32')\n",
    "                # new_shape = [input_shape[0], x2 - x1, input_shape[2]]\n",
    "                x_crop = x[:, x1:x2, :]\n",
    "                # x_crop = K.reshape(x_crop, new_shape)\n",
    "                if self.mode == \"max\":\n",
    "                    pooled_val = K.max(x_crop, axis=1)\n",
    "                elif self.mode == \"avg\":\n",
    "                    pooled_val = K.mean(x_crop, axis=1)\n",
    "                outputs.append(pooled_val)\n",
    "        outputs = K.concatenate(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unsupport dynamic input size\n",
    "class KMaxPooling1D(Layer):\n",
    "    def __init__(self, pool_list, mode = \"max\", **kwargs):\n",
    "        self.pool_list = np.array(pool_list)\n",
    "        self.mode = mode\n",
    "        assert self.pool_list.ndim == 1, \"pool_list ndim must be 1\"\n",
    "        assert self.mode in [\"max\", \"avg\"], \"mode must be either max or avg\"\n",
    "        self.num_outputs = sum(pool_list)\n",
    "        super(KMaxPooling1D, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.gram_size = input_shape[2]\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.num_outputs * self.gram_size)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'pool_list': self.pool_list, \"mode\": self.mode}\n",
    "        base_config = super(KMaxPooling1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    def call(self, x):\n",
    "        input_shape = x.get_shape()\n",
    "        input_shape_list = input_shape.as_list()\n",
    "        x = tf.reshape(x, [-1, input_shape[1], input_shape[2], tf.constant(1)])\n",
    "        outputs = []\n",
    "        gram_length = [input_shape_list[1] / i for i in self.pool_list]\n",
    "        embedding_size = input_shape_list[2]\n",
    "        for pool_index, num_pool_regions in enumerate(self.pool_list):\n",
    "            ph = np.round(gram_length[pool_index]).astype(np.int32)\n",
    "            sh = ph\n",
    "            if self.mode == \"max\":\n",
    "                pool_result = tf.nn.max_pool(x,\n",
    "                                             ksize=[1, ph, embedding_size, 1], \n",
    "                                             strides=[1, sh, 1, 1],\n",
    "                                             padding='SAME')\n",
    "            elif self.mode == \"avg\":\n",
    "                pool_result = tf.nn.avg_pool(x,\n",
    "                                             ksize=[1, ph, embedding_size, 1], \n",
    "                                             strides=[1, sh, 1, 1],\n",
    "                                             padding='SAME')\n",
    "            outputs.append(tf.reshape(pool_result, [-1, tf.constant(self.pool_list[pool_index]) * input_shape[2]]))\n",
    "        outputs = K.concatenate(outputs, axis=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGenerator(object):\n",
    "    def __init__(self, X, y, batch_size = 125, bins_count=100, mode=\"train\", onehot = False):\n",
    "        self.batch_size = batch_size\n",
    "        padding_mode = \"random\" if mode == \"train\" else \"specific\"\n",
    "        self.pool = preprocessing.AutoPaddingInMemorySamplePool(X, chunk_size=batch_size, bins_count=bins_count, mode=padding_mode)\n",
    "        self.y = y[self.pool.sorted_indices]\n",
    "        self.mode = mode\n",
    "        self.y_indices_record = []\n",
    "        self.onehot = onehot\n",
    "        if self.onehot:\n",
    "            onehot_encoder = OneHotEncoder()\n",
    "            self.y = self.y.reshape((-1, 1))\n",
    "            self.y = onehot_encoder.fit_transform(self.y).toarray()\n",
    "    \n",
    "    def iter(self):\n",
    "        return self\n",
    "    \n",
    "    def reset(self):\n",
    "        self.pool.reset()\n",
    "        self.y_indices_record = []\n",
    "    \n",
    "    def __next__(self):\n",
    "        batch_samples = self.pool.__next__()\n",
    "        if self.mode == \"test\":\n",
    "            self.y_indices_record.extend(self.pool.chunk_indices_list)\n",
    "        return batch_samples, self.y[self.pool.chunk_indices_list]\n",
    "    \n",
    "    def get_test_y(self, steps):\n",
    "        return self.y[self.y_indices_record[: steps * self.batch_size]]\n",
    "onehot = True\n",
    "SentGener_train = SentenceGenerator(batch_size=64, X=X_train[:22000], y=y_train[:22000], mode=\"train\", onehot=onehot)\n",
    "SentGener_val = SentenceGenerator(batch_size=64, X=X_train[22000:], y=y_train[22000:], mode=\"test\", bins_count=10, onehot=onehot)\n",
    "SentGener_test = SentenceGenerator(batch_size=64, X=X_test, y=y_test, mode=\"test\", onehot=onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 250 400 400 46\n"
     ]
    }
   ],
   "source": [
    "print(SentGener_train.pool.min_gap, SentGener_test.pool.min_gap, SentGener_train.pool.steps, SentGener_test.pool.steps, min(SentGener_train.pool.bins_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping('val_loss', patience=5, mode=\"min\"), ModelCheckpoint(\"E:/kaggle/avito/imdb_testset/tf_model/spp_net_imdb.hdf5\", save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spp_embeding_model(pool_list, num_words, mode = \"avg\", word_size = 100, embedings = None):\n",
    "    word_indices = Input(shape=[None], name=\"word_indices\")\n",
    "    if emdedings is None:\n",
    "        word_embedding = Embedding(num_words, word_size)(word_indices)\n",
    "    else:\n",
    "        word_embedding = Embedding(num_words, word_size, weights=[embedings], trainable=False)(word_indices)\n",
    "    x_flow = word_embedding\n",
    "    x_flow = Dropout(0.1)(word_embedding)\n",
    "    x_flow = Conv1D(512, 3, padding='same', activation='relu', strides=1)(x_flow)\n",
    "    x_flow = MaxPooling1D(3)(x_flow)\n",
    "    x_flow = Conv1D(256, 3, padding='same', activation='relu', strides=1)(x_flow)\n",
    "    x_flow = Conv1D(128, 3, padding='same', activation='relu', strides=1)(x_flow)\n",
    "    x_flow = SpatialPyramidPooling1D(pool_list=pool_list, mode=mode)(x_flow)\n",
    "    x_flow = Dropout(0.1)(x_flow)\n",
    "    x_flow = Dense(512, activation='relu')(x_flow)\n",
    "    x_flow = Dropout(0.1)(x_flow)\n",
    "    x_flow = Dense(256, activation='relu')(x_flow)\n",
    "    y_output = Dense(1, activation='sigmoid')(x_flow)\n",
    "    sgd = Adam(lr=1e-3)\n",
    "    model = Model(inputs=[word_indices], outputs=y_output)\n",
    "    model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def get_spp_embeding_model_shallow(pool_list, num_words, mode = \"avg\", word_size = 100, embedings = None):\n",
    "    word_indices = Input(shape=[None], name=\"word_indices\")\n",
    "    if embedings is None:\n",
    "        word_embedding = Embedding(num_words, word_size)(word_indices)\n",
    "    else:\n",
    "        word_embedding = Embedding(num_words, word_size, weights=[embedings], trainable=False)(word_indices)\n",
    "    x_flow = word_embedding\n",
    "    x_flow_a = Conv1D(128, 2, padding='same', activation='relu', kernel_regularizer=regularizers.l2(1e-3))(x_flow)\n",
    "    x_flow_b = Conv1D(128, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(1e-3))(x_flow)\n",
    "    x_flow_c = Conv1D(128, 4, padding='same', activation='relu', kernel_regularizer=regularizers.l2(1e-3))(x_flow)\n",
    "    x_flow_d = Conv1D(128, 5, padding='same', activation='relu', kernel_regularizer=regularizers.l2(1e-3))(x_flow)\n",
    "    x_flow = Concatenate(axis=1)([x_flow_a, x_flow_b, x_flow_c, x_flow_d])\n",
    "    x_flow = SpatialPyramidPooling1D(pool_list=pool_list, mode=mode)(x_flow)\n",
    "    x_flow = Dense(128, activation='relu')(x_flow)\n",
    "    y_output = Dense(2, activation='softmax')(x_flow)\n",
    "    sgd = Adam(lr=1e-3)\n",
    "    model = Model(inputs=[word_indices], outputs=y_output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed = spp_model.layers[1]\n",
    "# K.get_session().run(embed.embeddings)\n",
    "# embedings = K.get_session().run(spp_model.layers[1].embeddings\n",
    "# np.save(\"E:/kaggle/avito/imdb_testset/imdb_embedding_size_300_words_30000\", embedings)\n",
    "# embedings = np.load(\"E:/kaggle/avito/imdb_testset/imdb_embedding_size_300_words_30000.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word_indices (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, None, 300)    9000000     word_indices[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, None, 128)    76928       embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, None, 128)    115328      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, None, 128)    153728      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, None, 128)    192128      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, None, 128)    0           conv1d_20[0][0]                  \n",
      "                                                                 conv1d_21[0][0]                  \n",
      "                                                                 conv1d_22[0][0]                  \n",
      "                                                                 conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_pyramid_pooling1d_7 (Sp (None, 896)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 128)          114816      spatial_pyramid_pooling1d_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 2)            258         dense_13[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 9,653,186\n",
      "Trainable params: 9,653,186\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "spp_model = get_spp_embeding_model_shallow(pool_list=[1, 2, 4], num_words=30000, word_size=300, mode=\"max\", embedings=None)\n",
    "spp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "400/400 [==============================] - 24s 60ms/step - loss: 0.4775 - acc: 0.8307 - val_loss: 0.3608 - val_acc: 0.8713\n",
      "Epoch 2/20\n",
      "400/400 [==============================] - 22s 56ms/step - loss: 0.1817 - acc: 0.9548 - val_loss: 0.3083 - val_acc: 0.9027\n",
      "Epoch 3/20\n",
      "400/400 [==============================] - 21s 52ms/step - loss: 0.0925 - acc: 0.9864 - val_loss: 0.3390 - val_acc: 0.8967\n",
      "Epoch 4/20\n",
      "400/400 [==============================] - 20s 49ms/step - loss: 0.0621 - acc: 0.9937 - val_loss: 0.3764 - val_acc: 0.9010\n",
      "Epoch 5/20\n",
      "400/400 [==============================] - 22s 56ms/step - loss: 0.0321 - acc: 0.9990 - val_loss: 0.3409 - val_acc: 0.9030\n",
      "Epoch 6/20\n",
      "400/400 [==============================] - 22s 55ms/step - loss: 0.0173 - acc: 0.9998 - val_loss: 0.3434 - val_acc: 0.8990\n",
      "Epoch 7/20\n",
      "400/400 [==============================] - 22s 54ms/step - loss: 0.0138 - acc: 0.9991 - val_loss: 0.6037 - val_acc: 0.8693\n"
     ]
    }
   ],
   "source": [
    "callbacks[0].best = np.inf\n",
    "callbacks[1].best = np.inf\n",
    "spp_model.fit_generator(SentGener_train, steps_per_epoch=SentGener_train.pool.steps, epochs=20, shuffle=True, verbose=1, \n",
    "                        callbacks=callbacks,\n",
    "                        validation_data=SentGener_val,\n",
    "                        validation_steps=SentGener_val.pool.steps\n",
    "                       )\n",
    "spp_model.load_weights(\"E:/kaggle/avito/imdb_testset/tf_model/spp_net_imdb.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1/400 [..............................] - ETA: 2:46"
     ]
    }
   ],
   "source": [
    "SentGener_test.reset()\n",
    "test_y_hat = spp_model.predict_generator(SentGener_test, steps=SentGener_test.pool.steps, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(SentGener_test.y, np.round(test_y_hat + 1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
