{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Miniconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf \n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "import preprocessing\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'preprocessing' from 'D:\\\\jupyter-workdir\\\\nlp\\\\bg_fusion\\\\preprocessing.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import pickle\n",
    "wordCounter = preprocessing.WordCounter()\n",
    "if not osp.isfile(\"E:/kaggle/avito/imdb_testset/aclImdb_v1/words_counter_list\"):\n",
    "    wordCounter.fit([\"E:/kaggle/avito/imdb_testset/aclImdb_v1/train_test_neg.txt\", \"E:/kaggle/avito/imdb_testset/aclImdb_v1/train_test_pos.txt\"])\n",
    "    pickle.dump(wordCounter.words_list, open(\"E:/kaggle/avito/imdb_testset/aclImdb_v1/words_counter_list\", \"wb\"))\n",
    "else:\n",
    "    wordCounter.words_list = pickle.load(open(\"E:/kaggle/avito/imdb_testset/aclImdb_v1/words_counter_list\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words count 181924\n"
     ]
    }
   ],
   "source": [
    "print(\"words count\", len(wordCounter.words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdata(num_words=None):\n",
    "    state = np.random.RandomState(0)\n",
    "    X_train_pos = np.array(wordCounter.transform([\"E:/kaggle/avito/imdb_testset/aclImdb_v1/train_pos.txt\"], max_words=num_words))\n",
    "    X_train_neg = np.array(wordCounter.transform([\"E:/kaggle/avito/imdb_testset/aclImdb_v1/train_neg.txt\"], max_words=num_words))\n",
    "    y_train_pos = np.ones(X_train_pos.shape[0])\n",
    "    y_train_neg = np.zeros(X_train_neg.shape[0])\n",
    "    X_test_pos = np.array(wordCounter.transform([\"E:/kaggle/avito/imdb_testset/aclImdb_v1/test_pos.txt\"], max_words=num_words))\n",
    "    X_test_neg = np.array(wordCounter.transform([\"E:/kaggle/avito/imdb_testset/aclImdb_v1/test_neg.txt\"], max_words=num_words))\n",
    "    y_test_pos = np.ones(X_test_pos.shape[0])\n",
    "    y_test_neg = np.zeros(X_test_neg.shape[0])\n",
    "    X_train, y_train = np.concatenate([X_train_pos, X_train_neg]), np.concatenate([y_train_pos, y_train_neg])\n",
    "    X_test, y_test = np.concatenate([X_test_pos, X_test_neg]), np.concatenate([y_test_pos, y_test_neg])\n",
    "    train_permut = state.permutation(X_train.shape[0])\n",
    "    test_permut = state.permutation(X_test.shape[0])\n",
    "    return (X_train[train_permut], y_train[train_permut]), (X_test[test_permut], y_test[test_permut])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (X_train, y_train), (X_test, y_test) = imdb.load_data(path=\"E:/paper/stackingmodel/imdb/imdb.npz\", num_words=10000)\n",
    "(X_train, y_train), (X_test, y_test) = getdata(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min([len(x) for X in (X_train, X_test) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SpatialPyramidPooling1D(Layer):\n",
    "    def __init__(self, pool_list, mode = \"max\", **kwargs):\n",
    "        self.pool_list = np.array(pool_list)\n",
    "        self.mode = mode\n",
    "        assert self.pool_list.ndim == 1, \"pool_list ndim must be 1\"\n",
    "        assert self.mode in [\"max\", \"avg\"], \"mode must be either max or avg\"\n",
    "        self.num_outputs = sum(pool_list)\n",
    "        super(SpatialPyramidPooling1D, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.gram_size = input_shape[2]\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.num_outputs * self.gram_size)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'pool_list': self.pool_list, \"mode\": self.mode}\n",
    "        base_config = super(SpatialPyramidPooling1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    def call(self, x):\n",
    "        input_shape = K.shape(x)\n",
    "        gram_length = [K.cast(input_shape[1], 'float32') / i for i in self.pool_list]\n",
    "        outputs = []\n",
    "        for pool_index, num_pool_regions in enumerate(self.pool_list):\n",
    "            for ix in range(num_pool_regions):\n",
    "                x1 = ix * gram_length[pool_index]\n",
    "                x2 = ix * gram_length[pool_index] + gram_length[pool_index]\n",
    "                x1 = K.cast(K.round(x1), 'int32')\n",
    "                x2 = K.cast(K.round(x2), 'int32')\n",
    "                # new_shape = [input_shape[0], x2 - x1, input_shape[2]]\n",
    "                x_crop = x[:, x1:x2, :]\n",
    "                # x_crop = K.reshape(x_crop, new_shape)\n",
    "                if self.mode == \"max\":\n",
    "                    pooled_val = K.max(x_crop, axis=1)\n",
    "                elif self.mode == \"avg\":\n",
    "                    pooled_val = K.mean(x_crop, axis=1)\n",
    "                outputs.append(pooled_val)\n",
    "        outputs = K.concatenate(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unsupport dynamic input size\n",
    "class KMaxPooling1D(Layer):\n",
    "    def __init__(self, pool_list, mode = \"max\", **kwargs):\n",
    "        self.pool_list = np.array(pool_list)\n",
    "        self.mode = mode\n",
    "        assert self.pool_list.ndim == 1, \"pool_list ndim must be 1\"\n",
    "        assert self.mode in [\"max\", \"avg\"], \"mode must be either max or avg\"\n",
    "        self.num_outputs = sum(pool_list)\n",
    "        super(KMaxPooling1D, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.gram_size = input_shape[2]\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.num_outputs * self.gram_size)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'pool_list': self.pool_list, \"mode\": self.mode}\n",
    "        base_config = super(KMaxPooling1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    def call(self, x):\n",
    "        input_shape = x.get_shape()\n",
    "        input_shape_list = input_shape.as_list()\n",
    "        x = tf.reshape(x, [-1, input_shape[1], input_shape[2], tf.constant(1)])\n",
    "        outputs = []\n",
    "        gram_length = [input_shape_list[1] / i for i in self.pool_list]\n",
    "        embedding_size = input_shape_list[2]\n",
    "        for pool_index, num_pool_regions in enumerate(self.pool_list):\n",
    "            ph = np.round(gram_length[pool_index]).astype(np.int32)\n",
    "            sh = ph\n",
    "            if self.mode == \"max\":\n",
    "                pool_result = tf.nn.max_pool(x,\n",
    "                                             ksize=[1, ph, embedding_size, 1], \n",
    "                                             strides=[1, sh, 1, 1],\n",
    "                                             padding='SAME')\n",
    "            elif self.mode == \"avg\":\n",
    "                pool_result = tf.nn.avg_pool(x,\n",
    "                                             ksize=[1, ph, embedding_size, 1], \n",
    "                                             strides=[1, sh, 1, 1],\n",
    "                                             padding='SAME')\n",
    "            outputs.append(tf.reshape(pool_result, [-1, tf.constant(self.pool_list[pool_index]) * input_shape[2]]))\n",
    "        outputs = K.concatenate(outputs, axis=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentenceGenerator(object):\n",
    "    def __init__(self, X, y, batch_size = 125, bins_count=100, mode=\"train\"):\n",
    "        self.batch_size = batch_size\n",
    "        padding_mode = \"random\" if mode == \"train\" else \"specific\"\n",
    "        self.pool = preprocessing.AutoPaddingInMemorySamplePool(X, chunk_size=batch_size, bins_count=bins_count, mode=padding_mode)\n",
    "        self.y = y[self.pool.sorted_indices]\n",
    "        self.mode = mode\n",
    "        self.y_indices_record = []\n",
    "    \n",
    "    def iter(self):\n",
    "        return self\n",
    "    \n",
    "    def reset(self):\n",
    "        self.pool.reset()\n",
    "        self.y_indices_record = []\n",
    "    \n",
    "    def __next__(self):\n",
    "        batch_samples = self.pool.__next__()\n",
    "        if self.mode == \"test\":\n",
    "            self.y_indices_record.extend(self.pool.chunk_indices_list)\n",
    "        return batch_samples, self.y[self.pool.chunk_indices_list]\n",
    "    \n",
    "    def get_test_y(self, steps):\n",
    "        return self.y[self.y_indices_record[: steps * self.batch_size]]\n",
    "SentGener_train = SentenceGenerator(X_train[:22000], y_train[:22000])\n",
    "SentGener_val = SentenceGenerator(X_train[22000:], y_train[22000:], mode=\"test\", bins_count=10)\n",
    "SentGener_test = SentenceGenerator(X_test, y_test, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 250 200 200 46\n"
     ]
    }
   ],
   "source": [
    "print(SentGener_train.pool.min_gap, SentGener_test.pool.min_gap, SentGener_train.pool.steps, SentGener_test.pool.steps, min(SentGener_train.pool.bins_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping('val_loss', patience=5, mode=\"min\"), ModelCheckpoint(\"E:/kaggle/avito/imdb_testset/tf_model/spp_net_imdb.hdf5\", save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_spp_embeding_model(pool_list, num_words, mode = \"avg\", word_size = 100):\n",
    "    word_indices = Input(shape=[None], name=\"word_indices\")\n",
    "    word_embedding = Embedding(num_words, word_size)(word_indices)\n",
    "    x_flow = Dropout(0.1)(word_embedding)\n",
    "    x_flow = Conv1D(512, 3, padding='same', activation='relu', strides=1)(x_flow)\n",
    "    x_flow = MaxPooling1D(3, padding='valid')(x_flow)\n",
    "    x_flow = Conv1D(256, 3, padding='same', activation='relu', strides=1)(x_flow)\n",
    "    x_flow = Conv1D(128, 3, padding='same', activation='relu', strides=1)(x_flow)\n",
    "    x_flow = SpatialPyramidPooling1D(pool_list=pool_list, mode=mode)(x_flow)\n",
    "    x_flow = Dropout(0.1)(x_flow)\n",
    "    x_flow = Dense(512, activation='relu')(x_flow)\n",
    "    x_flow = Dropout(0.1)(x_flow)\n",
    "    x_flow = Dense(256, activation='relu')(x_flow)\n",
    "    y_output = Dense(1, activation='sigmoid')(x_flow)\n",
    "    sgd = Adam(lr=1e-3)\n",
    "#     sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model = Model(inputs=[word_indices], outputs=y_output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def get_kmaxpooling_embedding_model(pool_list, num_words, mode = \"avg\", word_size = 100):\n",
    "    word_indices = Input(shape=[1256], name=\"word_indices\")\n",
    "    word_embedding = Embedding(num_words, word_size)(word_indices)\n",
    "    x_flow = Dropout(0.1)(word_embedding)\n",
    "    x_flow = Conv1D(256, 3, padding='same', activation='relu', strides=1)(x_flow)\n",
    "    x_flow = MaxPooling1D(3)(x_flow)\n",
    "    x_flow = KMaxPooling1D(pool_list=pool_list, mode=mode)(x_flow)\n",
    "    x_flow = Dense(word_size, activation='relu')(x_flow)\n",
    "    y_output = Dense(1, activation='sigmoid')(x_flow)\n",
    "    sgd = Adam(lr=1e-3)\n",
    "    model = Model(inputs=[word_indices], outputs=y_output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed = spp_model.layers[1]\n",
    "# K.get_session().run(embed.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "word_indices (InputLayer)    (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (None, None, 200)         2000000   \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, None, 200)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, None, 512)         307712    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, None, 256)         393472    \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, None, 128)         98432     \n",
      "_________________________________________________________________\n",
      "spatial_pyramid_pooling1d_9  (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 512)               459264    \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 3,390,465\n",
      "Trainable params: 3,390,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "spp_model = get_spp_embeding_model(pool_list=[1, 2, 4], num_words=10000, word_size=200, mode=\"max\")\n",
    "spp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "200/200 [==============================] - 13s 66ms/step - loss: 0.4850 - acc: 0.7133 - val_loss: 0.3466 - val_acc: 0.8503\n",
      "Epoch 2/20\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 0.2040 - acc: 0.9208 - val_loss: 0.3732 - val_acc: 0.8373\n",
      "Epoch 3/20\n",
      "200/200 [==============================] - 12s 60ms/step - loss: 0.1192 - acc: 0.9576 - val_loss: 0.3213 - val_acc: 0.8847\n",
      "Epoch 4/20\n",
      "200/200 [==============================] - 12s 59ms/step - loss: 0.0782 - acc: 0.9737 - val_loss: 0.4088 - val_acc: 0.8877\n",
      "Epoch 5/20\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 0.0576 - acc: 0.9804 - val_loss: 0.3089 - val_acc: 0.8877\n",
      "Epoch 6/20\n",
      "200/200 [==============================] - 11s 56ms/step - loss: 0.0427 - acc: 0.9855 - val_loss: 0.4774 - val_acc: 0.8903\n",
      "Epoch 7/20\n",
      "200/200 [==============================] - 12s 60ms/step - loss: 0.0468 - acc: 0.9824 - val_loss: 0.5128 - val_acc: 0.8683\n",
      "Epoch 8/20\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 0.0279 - acc: 0.9905 - val_loss: 0.5762 - val_acc: 0.8873\n",
      "Epoch 9/20\n",
      "200/200 [==============================] - 10s 49ms/step - loss: 0.0106 - acc: 0.9966 - val_loss: 0.6732 - val_acc: 0.8870\n",
      "Epoch 10/20\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 0.0213 - acc: 0.9928 - val_loss: 0.7258 - val_acc: 0.8867\n"
     ]
    }
   ],
   "source": [
    "spp_model.fit_generator(SentGener_train, steps_per_epoch=np.ceil(X_train.shape[0] / SentGener_train.batch_size).astype(np.int32), epochs=20, shuffle=True, verbose=1, \n",
    "                        callbacks=callbacks,\n",
    "                        validation_data=SentGener_val,\n",
    "                        validation_steps=SentGener_val.pool.steps\n",
    "                       )\n",
    "spp_model.load_weights(\"E:/kaggle/avito/imdb_testset/tf_model/spp_net_imdb.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - ETA:  - 3s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "SentGener_test.reset()\n",
    "test_y_hat = spp_model.predict_generator(SentGener_test, steps=SentGener_test.pool.steps, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87208"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(SentGener_test.y, np.round(test_y_hat + 1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
