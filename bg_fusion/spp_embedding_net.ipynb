{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf \n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "import preprocessing\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'preprocessing' from 'D:\\\\jupyter-workdir\\\\nlp\\\\bg_fusion\\\\preprocessing.py'>"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(path=\"E:/paper/stackingmodel/imdb/imdb.npz\", num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min([len(x) for X in (X_train, X_test) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SpatialPyramidPooling1D(Layer):\n",
    "    def __init__(self, pool_list, mode = \"max\", **kwargs):\n",
    "        self.pool_list = np.array(pool_list)\n",
    "        self.mode = mode\n",
    "        assert self.pool_list.ndim == 1, \"pool_list ndim must be 1\"\n",
    "        assert self.mode in [\"max\", \"avg\"], \"mode must be either max or avg\"\n",
    "        self.num_outputs = sum(pool_list)\n",
    "        super(SpatialPyramidPooling1D, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.gram_size = input_shape[2]\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.num_outputs * self.gram_size)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'pool_list': self.pool_list, \"mode\": self.mode}\n",
    "        base_config = super(SpatialPyramidPooling1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    def call(self, x):\n",
    "        input_shape = K.shape(x)\n",
    "        gram_length = [K.cast(input_shape[1], 'float32') / i for i in self.pool_list]\n",
    "        outputs = []\n",
    "        for pool_index, num_pool_regions in enumerate(self.pool_list):\n",
    "            for ix in range(num_pool_regions):\n",
    "                x1 = ix * gram_length[pool_index]\n",
    "                x2 = ix * gram_length[pool_index] + gram_length[pool_index]\n",
    "                x1 = K.cast(K.round(x1), 'int32')\n",
    "                x2 = K.cast(K.round(x2), 'int32')\n",
    "                # new_shape = [input_shape[0], x2 - x1, input_shape[2]]\n",
    "                x_crop = x[:, x1:x2, :]\n",
    "                # x_crop = K.reshape(x_crop, new_shape)\n",
    "                if self.mode == \"max\":\n",
    "                    pooled_val = K.max(x_crop, axis=1)\n",
    "                elif self.mode == \"avg\":\n",
    "                    pooled_val = K.mean(x_crop, axis=1)\n",
    "                outputs.append(pooled_val)\n",
    "        outputs = K.concatenate(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unsupport dynamic input size\n",
    "class KMaxPooling1D(Layer):\n",
    "    def __init__(self, pool_list, mode = \"max\", **kwargs):\n",
    "        self.pool_list = np.array(pool_list)\n",
    "        self.mode = mode\n",
    "        assert self.pool_list.ndim == 1, \"pool_list ndim must be 1\"\n",
    "        assert self.mode in [\"max\", \"avg\"], \"mode must be either max or avg\"\n",
    "        self.num_outputs = sum(pool_list)\n",
    "        super(KMaxPooling1D, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.gram_size = input_shape[2]\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.num_outputs * self.gram_size)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'pool_list': self.pool_list, \"mode\": self.mode}\n",
    "        base_config = super(KMaxPooling1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    def call(self, x):\n",
    "        input_shape = x.get_shape()\n",
    "        input_shape_list = input_shape.as_list()\n",
    "        x = tf.reshape(x, [-1, input_shape[1], input_shape[2], tf.constant(1)])\n",
    "        outputs = []\n",
    "        gram_length = [input_shape_list[1] / i for i in self.pool_list]\n",
    "        embedding_size = input_shape_list[2]\n",
    "        for pool_index, num_pool_regions in enumerate(self.pool_list):\n",
    "            ph = np.round(gram_length[pool_index]).astype(np.int32)\n",
    "            sh = ph\n",
    "            if self.mode == \"max\":\n",
    "                pool_result = tf.nn.max_pool(x,\n",
    "                                             ksize=[1, ph, embedding_size, 1], \n",
    "                                             strides=[1, sh, 1, 1],\n",
    "                                             padding='SAME')\n",
    "            elif self.mode == \"avg\":\n",
    "                pool_result = tf.nn.avg_pool(x,\n",
    "                                             ksize=[1, ph, embedding_size, 1], \n",
    "                                             strides=[1, sh, 1, 1],\n",
    "                                             padding='SAME')\n",
    "            outputs.append(tf.reshape(pool_result, [-1, tf.constant(self.pool_list[pool_index]) * input_shape[2]]))\n",
    "        outputs = K.concatenate(outputs, axis=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGenerator(object):\n",
    "    def __init__(self, X, y, batch_size = 125, bins_count=100, mode=\"train\"):\n",
    "        self.batch_size = batch_size\n",
    "        padding_mode = \"random\" if mode == \"train\" else \"specific\"\n",
    "        self.pool = preprocessing.AutoPaddingInMemorySamplePool(X, chunk_size=batch_size, bins_count=bins_count, mode=padding_mode)\n",
    "        self.y = y[self.pool.sorted_indices]\n",
    "        self.mode = mode\n",
    "        self.y_indices_record = []\n",
    "    \n",
    "    def iter(self):\n",
    "        return self\n",
    "    \n",
    "    def reset(self):\n",
    "        self.pool.reset()\n",
    "        self.y_indices_record = []\n",
    "    \n",
    "    def __next__(self):\n",
    "        batch_samples = self.pool.__next__()\n",
    "        if self.mode == \"test\":\n",
    "            self.y_indices_record.extend(self.pool.chunk_indices_list)\n",
    "        return batch_samples, self.y[self.pool.chunk_indices_list]\n",
    "    \n",
    "    def get_test_y(self, steps):\n",
    "        return self.y[self.y_indices_record[: steps * self.batch_size]]\n",
    "SentGener_train = SentenceGenerator(X_train[:22000], y_train[:22000])\n",
    "SentGener_val = SentenceGenerator(X_train[22000:], y_train[22000:], mode=\"test\", bins_count=10)\n",
    "SentGener_test = SentenceGenerator(X_test, y_test, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 250 200 200 44\n"
     ]
    }
   ],
   "source": [
    "print(SentGener_train.pool.min_gap, SentGener_test.pool.min_gap, SentGener_train.pool.steps, SentGener_test.pool.steps, min(SentGener_train.pool.bins_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spp_embeding_model(pool_list, num_words, mode = \"avg\", word_size = 100):\n",
    "    word_indices = Input(shape=[None], name=\"word_indices\")\n",
    "    word_embedding = Embedding(num_words, word_size)(word_indices)\n",
    "    x_flow = Dropout(0.1)(word_embedding)\n",
    "    x_flow = Conv1D(512, 3, padding='same', activation='relu', strides=1)(x_flow)\n",
    "    x_flow = MaxPooling1D(3)(x_flow)\n",
    "    x_flow = Conv1D(256, 3, padding='same', activation='relu', strides=1)(x_flow)\n",
    "    x_flow = Conv1D(128, 3, padding='same', activation='relu', strides=1)(x_flow)\n",
    "    x_flow = SpatialPyramidPooling1D(pool_list=pool_list, mode=mode)(x_flow)\n",
    "    x_flow = Dropout(0.1)(x_flow)\n",
    "    x_flow = Dense(512, activation='relu')(x_flow)\n",
    "    x_flow = Dropout(0.1)(x_flow)\n",
    "    x_flow = Dense(256, activation='relu')(x_flow)\n",
    "    y_output = Dense(1, activation='sigmoid')(x_flow)\n",
    "    sgd = Adam(lr=1e-3)\n",
    "    model = Model(inputs=[word_indices], outputs=y_output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def get_kmaxpooling_embedding_model(pool_list, num_words, mode = \"avg\", word_size = 100):\n",
    "    word_indices = Input(shape=[1256], name=\"word_indices\")\n",
    "    word_embedding = Embedding(num_words, word_size)(word_indices)\n",
    "    x_flow = Dropout(0.1)(word_embedding)\n",
    "    x_flow = Conv1D(256, 3, padding='same', activation='relu', strides=1)(x_flow)\n",
    "    x_flow = MaxPooling1D(3)(x_flow)\n",
    "    x_flow = KMaxPooling1D(pool_list=pool_list, mode=mode)(x_flow)\n",
    "    x_flow = Dense(word_size, activation='relu')(x_flow)\n",
    "    y_output = Dense(1, activation='sigmoid')(x_flow)\n",
    "    sgd = Adam(lr=1e-3)\n",
    "    model = Model(inputs=[word_indices], outputs=y_output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "word_indices (InputLayer)    (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_55 (Embedding)     (None, None, 200)         2000000   \n",
      "_________________________________________________________________\n",
      "dropout_81 (Dropout)         (None, None, 200)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, None, 512)         307712    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_74 (MaxPooling (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, None, 256)         393472    \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, None, 128)         98432     \n",
      "_________________________________________________________________\n",
      "spatial_pyramid_pooling1d_40 (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 512)               459264    \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 3,390,465\n",
      "Trainable params: 3,390,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "spp_model = get_spp_embeding_model(pool_list=[1, 2, 4], num_words=10000, word_size=200)\n",
    "spp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping('val_loss', patience=10, mode=\"min\"), ModelCheckpoint(\"E:/kaggle/avito/imdb_testset/tf_model/spp_net_imdb.hdf5\", save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 0.4113 - acc: 0.7796 - val_loss: 0.2919 - val_acc: 0.8890\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 11s 55ms/step - loss: 0.1929 - acc: 0.9298 - val_loss: 0.2888 - val_acc: 0.8910\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 12s 60ms/step - loss: 0.1075 - acc: 0.9658 - val_loss: 0.3464 - val_acc: 0.8820\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 12s 59ms/step - loss: 0.0732 - acc: 0.9764 - val_loss: 0.4354 - val_acc: 0.8810\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 11s 55ms/step - loss: 0.0596 - acc: 0.9816 - val_loss: 0.4292 - val_acc: 0.8690\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 11s 56ms/step - loss: 0.0318 - acc: 0.9896 - val_loss: 0.6159 - val_acc: 0.8597\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.0205 - acc: 0.9938 - val_loss: 0.6266 - val_acc: 0.8830\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 12s 60ms/step - loss: 0.0177 - acc: 0.9941 - val_loss: 0.6854 - val_acc: 0.8707\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 11s 56ms/step - loss: 0.0285 - acc: 0.9907 - val_loss: 0.8698 - val_acc: 0.8713\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 0.0181 - acc: 0.9942 - val_loss: 0.7113 - val_acc: 0.8870\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 10s 52ms/step - loss: 0.0144 - acc: 0.9953 - val_loss: 0.9473 - val_acc: 0.8700\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 12s 58ms/step - loss: 0.0076 - acc: 0.9975 - val_loss: 0.8241 - val_acc: 0.8810\n"
     ]
    }
   ],
   "source": [
    "spp_model.fit_generator(SentGener_train, steps_per_epoch=np.ceil(X_train.shape[0] / SentGener_train.batch_size).astype(np.int32), epochs=100, shuffle=True, verbose=1, \n",
    "                        callbacks=callbacks,\n",
    "                        validation_data=SentGener_val,\n",
    "                        validation_steps=SentGener_val.pool.steps\n",
    "                       )\n",
    "spp_model.load_weights(\"E:/kaggle/avito/imdb_testset/tf_model/spp_net_imdb.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 4s 22ms/step\n"
     ]
    }
   ],
   "source": [
    "SentGener_test.reset()\n",
    "test_y_hat = spp_model.predict_generator(SentGener_test, steps=SentGener_test.pool.steps, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8778"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(SentGener_test.y, np.round(test_y_hat + 1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
