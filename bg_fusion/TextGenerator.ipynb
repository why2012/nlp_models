{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Miniconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "D:\\Miniconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'Visualization' from 'D:\\\\jupyter-workdir\\\\nlp\\\\bg_fusion\\\\Visualization.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf \n",
    "import keras\n",
    "import gensim\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization, Add, Flatten, LSTM, GlobalAveragePooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Concatenate, AveragePooling1D, Lambda, Multiply, GlobalMaxPooling1D\n",
    "from keras.layers import GRUCell, LSTMCell, RNN\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam, RMSprop, SGD, Adadelta\n",
    "from keras.models import Model, load_model\n",
    "from keras import regularizers\n",
    "from keras.preprocessing import sequence\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "import preprocessing\n",
    "import Visualization as vis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from collections import defaultdict, namedtuple\n",
    "import logging\n",
    "logger = logging.getLogger(\"wiki_word2vec\")\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "import os.path as osp\n",
    "import pickle\n",
    "\n",
    "import importlib\n",
    "importlib.reload(preprocessing)\n",
    "importlib.reload(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounter = preprocessing.WordCounter()\n",
    "wordCounter.words_list = pickle.load(open(\"E:/kaggle/avito/imdb_testset/aclImdb_v1/words_counter_list\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_word_count = len(wordCounter.words_list)\n",
    "num_words = 30000\n",
    "embed_size = 300\n",
    "batch_size = 100\n",
    "bins_count = 200\n",
    "pad_maxlen = 100\n",
    "sequence_len = 30\n",
    "rnn_size = 512\n",
    "rnn_layer_num = 3\n",
    "epochs = 20\n",
    "y_onehot = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedings = None\n",
    "if False:\n",
    "    WordVectorModel = namedtuple(\"WordVectorModel\", [\"wv\"])\n",
    "    wv = gensim.models.KeyedVectors.load_word2vec_format(\"E:/kaggle/avito/imdb_testset/dict2vec/dict2vec-vectors-dim300.vec.bin\", binary=True)\n",
    "    model = WordVectorModel(wv=wv)\n",
    "    embedings, not_in_list = wordCounter.get_pretrain_embedding(model, num_words=num_words, size=embed_size)\n",
    "    del wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicRNN(Layer):\n",
    "    def __init__(self, units, n_layers, input_keep_prob = 1.0, output_keep_prob = 1.0, hidden_state = None, reuse = True, unroll = False, **kwargs):\n",
    "        super(DynamicRNN, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.n_layers = n_layers\n",
    "        self.input_keep_prob = input_keep_prob\n",
    "        self.output_keep_prob = output_keep_prob\n",
    "        self.hidden_state = hidden_state\n",
    "        self.reuse = reuse\n",
    "        self.unroll = unroll\n",
    "        self.i_built = False\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.cells = [tf.contrib.rnn.GRUCell(self.units, reuse=self.reuse) for _ in range(self.n_layers)]\n",
    "        self.dropcells = [tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=self.input_keep_prob) for cell in self.cells]\n",
    "        self.multicell = tf.contrib.rnn.MultiRNNCell(self.dropcells, state_is_tuple=False)\n",
    "        self.multicell_wrapper = tf.contrib.rnn.DropoutWrapper(self.multicell, output_keep_prob=self.output_keep_prob)  \n",
    "        super(DynamicRNN, self).build(input_shape)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = (input_shape[0], input_shape[1], self.units)\n",
    "        state_shape = (input_shape[0], self.multicell.state_size)\n",
    "        return [output_shape, state_shape]\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\"units\": self.units, \"n_layers\": self.n_layers, \"input_keep_prob\": self.input_keep_prob, \n",
    "                  \"output_keep_prob\": self.output_keep_prob, \"reuse\": self.reuse, \"unroll\": self.unroll}\n",
    "        base_config = super(DynamicRNN, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    def get_initial_state(self, inputs):\n",
    "        if self.hidden_state is None:\n",
    "            # build an all-zero tensor of shape (samples, output_dim)\n",
    "            initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n",
    "            initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n",
    "            initial_state = K.expand_dims(initial_state)  # (samples, 1)\n",
    "            return K.tile(initial_state, [1, self.multicell.state_size])\n",
    "        else:\n",
    "            return self.hidden_state\n",
    "    \n",
    "    def call(self, x):\n",
    "        if self.unroll:\n",
    "            sequence_len = x.shape[1].value\n",
    "            state = self.get_initial_state(x)\n",
    "            Yr = [] # [ SEQLEN, BATCHSIZE, INTERNALSIZE ]\n",
    "            for unroll_step in range(sequence_len):\n",
    "                cell_output, state = self.multicell_wrapper(x[:, unroll_step, :], state)\n",
    "                Yr.append(cell_output)\n",
    "            Yr = tf.convert_to_tensor(Yr)\n",
    "            Yr = tf.transpose(Yr, [1, 0, 2])\n",
    "            H = state # [ BATCHSIZE, INTERNALSIZE *  NLAYERS ]\n",
    "        else:\n",
    "            Yr, H = tf.nn.dynamic_rnn(self.multicell_wrapper, inputs=x, dtype=tf.float32, initial_state=self.get_initial_state(x))\n",
    "        if not self.i_built:\n",
    "            self.i_built = True\n",
    "            self._trainable_weights.extend(self.multicell.weights)\n",
    "        # Yr: [ BATCHSIZE, SEQLEN, INTERNALSIZE ]\n",
    "        # H:   # this is the last state in the sequence\n",
    "        return [Yr, H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getdata(num_words=None):\n",
    "    state = np.random.RandomState(0)\n",
    "    X_train_pos = np.array(wordCounter.transform([\"E:/kaggle/avito/imdb_testset/aclImdb_v1/train_pos.txt\"], max_words=num_words))\n",
    "    X_train_neg = np.array(wordCounter.transform([\"E:/kaggle/avito/imdb_testset/aclImdb_v1/train_neg.txt\"], max_words=num_words))\n",
    "    y_train_pos = np.ones(X_train_pos.shape[0])\n",
    "    y_train_neg = np.zeros(X_train_neg.shape[0])\n",
    "    X_test_pos = np.array(wordCounter.transform([\"E:/kaggle/avito/imdb_testset/aclImdb_v1/test_pos.txt\"], max_words=num_words))\n",
    "    X_test_neg = np.array(wordCounter.transform([\"E:/kaggle/avito/imdb_testset/aclImdb_v1/test_neg.txt\"], max_words=num_words))\n",
    "    y_test_pos = np.ones(X_test_pos.shape[0])\n",
    "    y_test_neg = np.zeros(X_test_neg.shape[0])\n",
    "    X_train, y_train = np.concatenate([X_train_pos, X_train_neg]), np.concatenate([y_train_pos, y_train_neg])\n",
    "    X_test, y_test = np.concatenate([X_test_pos, X_test_neg]), np.concatenate([y_test_pos, y_test_neg])\n",
    "    train_permut = state.permutation(X_train.shape[0])\n",
    "    test_permut = state.permutation(X_test.shape[0])\n",
    "    \n",
    "    X_train_unsup = np.array(wordCounter.transform([\"E:/kaggle/avito/imdb_testset/aclImdb_v1/train_unsup.txt\"], max_words=num_words))\n",
    "    unsup_permut = state.permutation(X_train_unsup.shape[0])\n",
    "    \n",
    "    return (X_train[train_permut], y_train[train_permut]), (X_test[test_permut], y_test[test_permut]), X_train_unsup[unsup_permut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gather_texts(docs_list, n_words_per_unit = 20, padding_ch = 0):\n",
    "    units = []\n",
    "    for i, docs in enumerate(docs_list):\n",
    "        print(\"processing doc-%s\" % i)\n",
    "        for doc in docs:\n",
    "            units.extend(doc)\n",
    "    return units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_o, y_train), (X_test_o, y_test), X_train_unsup_o = getdata(num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing doc-0\n",
      "processing doc-1\n",
      "processing doc-2\n"
     ]
    }
   ],
   "source": [
    "X_all = gather_texts([X_train_o, X_test_o, X_train_unsup_o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train = sequence.pad_sequences(X_train_o, maxlen=pad_maxlen, padding='post', truncating='post')\n",
    "# X_test = sequence.pad_sequences(X_test_o, maxlen=pad_maxlen, padding='post', truncating='post')\n",
    "# X_train_unsup = sequence.pad_sequences(X_train_unsup_o, maxlen=pad_maxlen, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train = X_train_o\n",
    "# X_test = X_test_o\n",
    "# X_train_unsup = X_train_unsup_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_steps(raw_data, batch_size, sequence_size):\n",
    "    data_len = len(raw_data)\n",
    "    nb_batches = (data_len - 1) // (batch_size * sequence_size)\n",
    "    return nb_batches\n",
    "\n",
    "def rnn_minibatch_sequencer(raw_data, batch_size, sequence_size, nb_epochs):\n",
    "    \"\"\"\n",
    "    Divides the data into batches of sequences so that all the sequences in one batch\n",
    "    continue in the next batch. This is a generator that will keep returning batches\n",
    "    until the input data has been seen nb_epochs times. Sequences are continued even\n",
    "    between epochs, apart from one, the one corresponding to the end of raw_data.\n",
    "    The remainder at the end of raw_data that does not fit in an full batch is ignored.\n",
    "    :param raw_data: the training text\n",
    "    :param batch_size: the size of a training minibatch\n",
    "    :param sequence_size: the unroll size of the RNN\n",
    "    :param nb_epochs: number of epochs to train on\n",
    "    :return:\n",
    "        x: one batch of training sequences\n",
    "        y: on batch of target sequences, i.e. training sequences shifted by 1\n",
    "        epoch: the current epoch number (starting at 0)\n",
    "    \"\"\"\n",
    "    data = np.array(raw_data)\n",
    "    data_len = data.shape[0]\n",
    "    # using (data_len-1) because we must provide for the sequence shifted by 1 too\n",
    "    nb_batches = (data_len - 1) // (batch_size * sequence_size)\n",
    "    assert nb_batches > 0, \"Not enough data, even for a single batch. Try using a smaller batch_size.\"\n",
    "    rounded_data_len = nb_batches * batch_size * sequence_size\n",
    "    xdata = np.reshape(data[0:rounded_data_len], [batch_size, nb_batches * sequence_size])\n",
    "    ydata = np.reshape(data[1:rounded_data_len + 1], [batch_size, nb_batches * sequence_size])\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        for batch in range(nb_batches):\n",
    "            x = xdata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            y = ydata[:, batch * sequence_size:(batch + 1) * sequence_size]\n",
    "            x = np.roll(x, -epoch, axis=0)  # to continue the text from epoch to epoch (do not reset rnn state!)\n",
    "            y = np.roll(y, -epoch, axis=0)\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGenerator(object):\n",
    "    def __init__(self, X, y = None, batch_size = 125, bins_count=100, mode=\"train\", onehot = False):\n",
    "        self.batch_size = batch_size\n",
    "        padding_mode = \"random\" if mode == \"train\" else \"specific\"\n",
    "        self.pool = preprocessing.AutoPaddingInMemorySamplePool(X, chunk_size=batch_size, bins_count=bins_count, mode=padding_mode)\n",
    "        self.y = y\n",
    "        self.gen_mode = 0\n",
    "        if self.y is not None:\n",
    "            self.y = y[self.pool.sorted_indices]\n",
    "        else:\n",
    "            self.gen_mode = 1\n",
    "        self.mode = mode\n",
    "        self.y_indices_record = []\n",
    "        self.onehot = onehot\n",
    "        if self.onehot and self.y is not None:\n",
    "            onehot_encoder = OneHotEncoder()\n",
    "            self.y = self.y.reshape((-1, 1))\n",
    "            self.y = onehot_encoder.fit_transform(self.y).toarray()\n",
    "    \n",
    "    def iter(self):\n",
    "        return self\n",
    "    \n",
    "    def reset(self):\n",
    "        self.pool.reset()\n",
    "        self.y_indices_record = []\n",
    "    \n",
    "    def __next__(self):\n",
    "        batch_samples = self.pool.__next__()\n",
    "        if self.y is not None:\n",
    "            if self.mode == \"test\":\n",
    "                self.y_indices_record.extend(self.pool.chunk_indices_list)\n",
    "            return batch_samples, self.y[self.pool.chunk_indices_list]\n",
    "        else:\n",
    "            n_samples, n_cols = batch_samples.shape\n",
    "            # generate text target\n",
    "            # got w0, predict w1\n",
    "            y_batch_samples = np.roll(batch_samples, -1, axis=1)\n",
    "            # drop last word\n",
    "            batch_samples = batch_samples[:, 0: n_cols - 1]\n",
    "            y_batch_samples = y_batch_samples[:, 0: n_cols - 1]\n",
    "            return batch_samples, y_batch_samples\n",
    "    \n",
    "    def get_test_y(self, steps):\n",
    "        if self.y is not None:\n",
    "            return self.y[self.y_indices_record[: steps * self.batch_size]]\n",
    "        else:\n",
    "            return None\n",
    "if False:\n",
    "    SentGener_train_all = SentenceGenerator(batch_size=batch_size, X=np.concatenate([X_train, X_test, X_train_unsup]), y=None, mode=\"train\", bins_count=bins_count, onehot=y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, mode=\"min\"), \n",
    "    ModelCheckpoint(monitor=\"val_loss\", filepath=\"E:/kaggle/avito/imdb_testset/tf_model/generator_net_imdb.hdf5\", save_best_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=5, min_lr=1e-5, verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nce_loss_for_keras(vocab_size, rnn_size, model_input, contrast_num_samples=5):\n",
    "    nce_weights = tf.Variable(tf.truncated_normal([vocab_size, rnn_size], stddev=1.0 / np.sqrt(rnn_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocab_size]))\n",
    "    def get_nce_loss(y_true, y_pred):\n",
    "        # y_true shape: (batch_size, sequence_len)\n",
    "        # y_pred shape: (batch_size * sequence_len, rnn_size)\n",
    "        y_true = K.reshape(y_true, [-1, 1]) # shape (batch_size * sequence_len, 1)\n",
    "        nce_mean_loss = tf.reduce_mean(tf.nn.nce_loss(\n",
    "                                             weights=nce_weights,\n",
    "                                             biases=nce_biases,\n",
    "                                             labels=y_true,\n",
    "                                             inputs=y_pred,\n",
    "                                             num_sampled=contrast_num_samples,\n",
    "                                             num_classes=vocab_size))\n",
    "        return nce_mean_loss\n",
    "    \n",
    "    def nce_accuracy(y_true, y_pred):\n",
    "        # y_true shape: (batch_size, sequence_len)\n",
    "        # y_pred shape: (batch_size * sequence_len, rnn_size)\n",
    "        y_true = K.reshape(y_true, [-1]) # shape (batch_size * sequence_len, )\n",
    "        word_logits = tf.matmul(y_pred, tf.transpose(nce_weights)) # shape (batch_size * sequence_len, vocab_size)\n",
    "        word_logits = tf.nn.bias_add(word_logits, nce_biases) # shape (batch_size * sequence_len, vocab_size)\n",
    "        word_logits = tf.argmax(word_logits, axis=-1) # shape (batch_size * sequence_len, )\n",
    "        y_true = tf.cast(y_true, tf.int32) # float32 -> int32\n",
    "        word_logits = tf.cast(word_logits, tf.int32) # int64 -> int32\n",
    "        return tf.cast(tf.equal(y_true, word_logits), tf.float32)\n",
    "        \n",
    "    # model_input shape (batch_size * sequence_len, rnn_size)\n",
    "    word_logits = tf.matmul(model_input, tf.transpose(nce_weights)) # shape (batch_size * sequence_len, vocab_size)\n",
    "    word_logits = tf.nn.bias_add(word_logits, nce_biases) # shape (batch_size * sequence_len, vocab_size)\n",
    "    word_logits = tf.argmax(word_logits, axis=-1) # shape (batch_size * sequence_len, 1)\n",
    "    return get_nce_loss, nce_accuracy, word_logits\n",
    "\n",
    "def reset_graph():\n",
    "    tf.reset_default_graph()\n",
    "    K.get_session().close()\n",
    "    K.set_session(tf.Session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_generator_net(num_words, embed_size = 100, rnn_size = 512, rnn_layer_num = 3, contrast_num_samples = 64, embedings = None):\n",
    "    word_indices = Input(shape=[None], name=\"word_indices\") # shape (batch_size, sequence_len)\n",
    "    hidden_state = Input(shape=[rnn_layer_num * rnn_size], name=\"rnn_hidden_state\") # shape (batch_size, rnn_layer_num * rnn_size)\n",
    "    if embedings is None:\n",
    "        word_embedding = Embedding(num_words, embed_size, name=\"word_embedding\")(word_indices)\n",
    "    else:\n",
    "        word_embedding = Embedding(num_words, embed_size, weights=[embedings], name=\"word_embedding\", trainable=False)(word_indices)\n",
    "    x_flow = word_embedding # shape (batch_size, sequence_len, embed_size) (None, None, embed_size)\n",
    "    rnn_outputs = DynamicRNN(units=rnn_size, n_layers=rnn_layer_num, input_keep_prob = 1.0, output_keep_prob = 0.8, hidden_state=hidden_state, reuse=tf.AUTO_REUSE)(x_flow)\n",
    "    x_flow = rnn_outputs[0] # shape (batch_size, sequence_len, rnn_size)\n",
    "    rnn_states = rnn_outputs[1] # shape (batch_size, rnn_size * rnn_layer_num)\n",
    "    # x_flow = Lambda(lambda x: K.reshape(x, [-1, rnn_size]))(x_flow) # shape (batch_size * sequence_len, rnn_size)\n",
    "    # nce_loss, accuracy, word_logits = nce_loss_for_keras(vocab_size=num_words, rnn_size=rnn_size, contrast_num_samples=contrast_num_samples, model_input=x_flow)\n",
    "    x_flow = Dense(num_words, activation='softmax')(x_flow) # shape (batch_size, sequence_len, vocab_size)\n",
    "    sgd = Adam(lr=1e-3)\n",
    "    model = Model(inputs=[word_indices, hidden_state], outputs=x_flow)\n",
    "    # model.word_logits = word_logits\n",
    "    model.rnn_states = rnn_states\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=sgd, metrics=[\"acc\"])\n",
    "    generator_model = Model(inputs=[word_indices, hidden_state], outputs=[x_flow, rnn_states])\n",
    "    return model, generator_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_net_model, generator_model = get_text_generator_net(num_words=num_words, rnn_size=rnn_size, rnn_layer_num=rnn_layer_num, embed_size=embed_size, embedings=embedings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "word_indices (InputLayer)    (None, None)              0         \n",
      "_________________________________________________________________\n",
      "word_embedding (Embedding)   (None, None, 300)         9000000   \n",
      "_________________________________________________________________\n",
      "dynamic_rnn_1 (DynamicRNN)   [(None, None, 512), (None 4397568   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 30000)       15390000  \n",
      "=================================================================\n",
      "Total params: 28,787,568\n",
      "Trainable params: 28,787,568\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_net_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_with_rnnstate(sentGener, batch_size, rnn_layer_num, rnn_size):\n",
    "    rnnstate = np.zeros((batch_size, rnn_layer_num * rnn_size))\n",
    "    while True:\n",
    "        X, y = next(sentGener)\n",
    "        # X shape (batch_size, seq_len)\n",
    "        # Y shape (batch_size, seq_len)\n",
    "        y = np.expand_dims(y, 2)\n",
    "        # Y shape (batch_size, seq_len, 1)\n",
    "        yield [X, rnnstate], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2732/2732 [==============================] - 882s 323ms/step - loss: 4.9475 - acc: 0.2094\n",
      "Epoch 2/20\n",
      "2732/2732 [==============================] - 883s 323ms/step - loss: 4.7085 - acc: 0.2242\n",
      "Epoch 3/20\n",
      "2732/2732 [==============================] - 880s 322ms/step - loss: 4.6321 - acc: 0.2301\n",
      "Epoch 4/20\n",
      "2732/2732 [==============================] - 882s 323ms/step - loss: 4.5701 - acc: 0.2345\n",
      "Epoch 5/20\n",
      "2732/2732 [==============================] - 882s 323ms/step - loss: 4.5276 - acc: 0.2376\n",
      "Epoch 6/20\n",
      "2732/2732 [==============================] - 880s 322ms/step - loss: 4.4971 - acc: 0.2394\n",
      "Epoch 7/20\n",
      "2732/2732 [==============================] - 883s 323ms/step - loss: 4.4734 - acc: 0.2414\n",
      "Epoch 8/20\n",
      "2732/2732 [==============================] - 881s 323ms/step - loss: 4.4496 - acc: 0.2433\n",
      "Epoch 9/20\n",
      "2732/2732 [==============================] - 885s 324ms/step - loss: 4.4345 - acc: 0.2439\n",
      "Epoch 10/20\n",
      "2732/2732 [==============================] - 881s 322ms/step - loss: 4.4184 - acc: 0.2455\n",
      "Epoch 11/20\n",
      "2732/2732 [==============================] - 886s 324ms/step - loss: 4.4061 - acc: 0.2462\n",
      "Epoch 12/20\n",
      "2732/2732 [==============================] - 888s 325ms/step - loss: 4.3951 - acc: 0.2466\n",
      "Epoch 13/20\n",
      "2732/2732 [==============================] - 887s 325ms/step - loss: 4.3875 - acc: 0.2475\n",
      "Epoch 14/20\n",
      "2732/2732 [==============================] - 888s 325ms/step - loss: 4.3745 - acc: 0.2481\n",
      "Epoch 15/20\n",
      "2732/2732 [==============================] - 886s 324ms/step - loss: 4.3691 - acc: 0.2486\n",
      "Epoch 16/20\n",
      "2732/2732 [==============================] - 863s 316ms/step - loss: 4.3606 - acc: 0.2487\n",
      "Epoch 17/20\n",
      "2732/2732 [==============================] - 847s 310ms/step - loss: 4.3540 - acc: 0.2496\n",
      "Epoch 18/20\n",
      "2732/2732 [==============================] - 846s 310ms/step - loss: 4.3467 - acc: 0.2501\n",
      "Epoch 19/20\n",
      "2732/2732 [==============================] - 852s 312ms/step - loss: 4.3427 - acc: 0.2501\n",
      "Epoch 20/20\n",
      "2732/2732 [==============================] - 852s 312ms/step - loss: 4.3345 - acc: 0.2508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20622f56898>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks[1].best = -np.inf\n",
    "callbacks[2].best = -np.inf\n",
    "generator = generator_with_rnnstate(rnn_minibatch_sequencer(X_all, batch_size, sequence_size=sequence_len, nb_epochs=epochs), batch_size, rnn_layer_num, rnn_size)\n",
    "steps = get_steps(X_all, batch_size, sequence_size=batch_size)\n",
    "generator_net_model.fit_generator(generator, steps_per_epoch=steps, epochs=epochs, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator_net_model.layers[2].reuse = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_net_model.load_weights(\"E:/kaggle/avito/imdb_testset/tf_model/generator_net_imdb_type-A.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounter.i2w_dict = wordCounter.get_i2w_dictionary(num_words=num_words)\n",
    "wordCounter.w2i_dict = wordCounter.get_w2i_dictionary(num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_text_sequence(model, start_word, max_seq_len, vocab_size, wordCounter, rnn_layer_num, rnn_size):\n",
    "    start_word = start_word.strip().lower()\n",
    "    i2w_dict = wordCounter.i2w_dict\n",
    "    w2i_dict = wordCounter.w2i_dict\n",
    "    assert start_word in w2i_dict, \"%s not in word_dict\" % start_word\n",
    "    start_word_indice = np.zeros((1, 1), dtype=np.int32) # (batch_size, seq_len)\n",
    "    start_word_indice[0, 0] = w2i_dict[start_word]\n",
    "    rnn_state = np.zeros((1, rnn_layer_num * rnn_size))\n",
    "    generated_sequence = [w2i_dict[start_word]]\n",
    "    for step_i in range(max_seq_len):\n",
    "        y, rnn_state = model.predict([start_word_indice, rnn_state])\n",
    "        # y (batch_size, seq_len, vocab_size)\n",
    "        # rnn_state (batch_size, rnn_size * rnn_layer_num)\n",
    "        predict_word_index = np.argmax(y[0][0])\n",
    "        start_word_indice[0, 0] = predict_word_index\n",
    "        # rnn_state = K.get_session().run(model.rnn_states, feed_dict={model.get_input_at(0)[0]: start_word_indice, model.get_input_at(0)[1]: rnn_state})\n",
    "        generated_sequence.append(predict_word_index)\n",
    "    return wordCounter.reverse([generated_sequence], num_words=vocab_size, return_list=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"i ' m sure you ' ll be disappointed . i ' ve seen a lot of movies , but this one is just plain bad . i ' m not a big fan of the carpenters , but this movie is just plain bad . i don ' t know if it ' s a bad movie , but i ' m sure it ' s a great movie . i ' ve seen a lot of bad movies but i ' ve seen a lot of bad movies . i ' ve seen a lot of bad movies and i ' ve seen a lot of stinkers but this one is just plain bad . i ' ve seen worse movies in my time , but this one is just plain bad . i ' m not a big fan of the original but i ' ve seen better movies . i ' ve seen a lot of bad movies and i ' ve seen a lot of stinkers . i ' ve always liked the unk and the ghoulies . i ' ve always liked the original and the original . i ' ve seen a lot of\"]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_text_sequence(generator_model, \"i\", 200, num_words, wordCounter, rnn_layer_num, rnn_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
